{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ArjunNarayan2066/CS484_project/blob/fixed_training_loop/cs484_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-supervised depth estimation consists on training a neural network to provide dense pixel-wise depth predictions given a single image. \"Self-supervision\"  in the training process implies not relying on ground truth depth maps, which can be hard and expensive to acquire. This project is motivated by Digging Into Self-Supervised Monocular Depth Estimation [1].\n",
    "\n",
    "The network attempts to generate a synthetic view of the same scene from a different point of view. This implicitly generated a disparity map, if per-pixel correspondences are known. The relative pose of the virtual viewpoint with respect to the actual viewpoint (i.e. real camera) is represented as $T_{t' \\rightarrow t}$, and the synthetic image is denoted $I_{t'}$, given an original image $I_t$.\n",
    "\n",
    "The loss functions employed in the training are as follows. \n",
    "$$L_p = \\Sigma_{t'} pe( I_{t}, I_{t'\\rightarrow t})$$\n",
    "\n",
    "where $pe$ represents the $L1$ distance between matching pixels in pixel space and is defined as:\n",
    "\n",
    "$$pe(I_a, I_b) = \\frac{\\alpha}{2}(1-SSIM(I_a, I_b)) + (1-\\alpha) ||I_a - I_b||_1$$\n",
    "\n",
    "$\\alpha$ is set to 0.85.\n",
    "\n",
    "$SSIM$ represents structural similarity between images $I_a$ and $I_b$. That is, the sum of x and y distances for all matches:\n",
    "\n",
    "$$SSIM(x, y) = \\frac{(2\\mu_x\\mu_y+c_1)(2\\sigma_{xy}+c_2)}{(\\mu_x^2+\\mu_y^2+c_1)(\\sigma_x^2+\\sigma_y^2+c_2)}$$\n",
    "\n",
    "$I_{t'\\rightarrow t}$ is defined as follows:\n",
    "\n",
    "$$I_{t'\\rightarrow t} = I_{t'}\\langle proj(D_t, T_{t\\rightarrow t'}, K)\\rangle$$\n",
    "\n",
    "for $D_t$ being projected depths, $\\langle . \\rangle$ being the sampling operator.\n",
    "\n",
    "Note that the $pe$ function provides a weighted sum of visually perceptible differences between images (SSIM), and the L1 norm between said images, as described in [2].\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JnRlwbQ9z4kL",
    "outputId": "ad36e8f0-ac3b-4fce-90ff-462feb273ebd"
   },
   "outputs": [],
   "source": [
    "! nvcc --version\n",
    "\n",
    "! pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "! pip install tensorboardX==1.4\n",
    "! pip install opencv-python==3.3.1.11\n",
    "! pip install kornia\n",
    "\n",
    "# Clone repo\n",
    "! git clone https://github.com/ArjunNarayan2066/monodepth2.gitpth2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AK6si-eI1Owv"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import PIL.Image as pil\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import kornia\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "import networks\n",
    "from utils import download_model_if_doesnt_exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GyVajA6BA8CM"
   },
   "outputs": [],
   "source": [
    "class SSIM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SSIM, self).__init__()\n",
    "        self.mu_x_pool   = nn.AvgPool2d(3, 1)\n",
    "        self.mu_y_pool   = nn.AvgPool2d(3, 1)\n",
    "        self.sig_x_pool  = nn.AvgPool2d(3, 1)\n",
    "        self.sig_y_pool  = nn.AvgPool2d(3, 1)\n",
    "        self.sig_xy_pool = nn.AvgPool2d(3, 1)\n",
    "\n",
    "        self.refl = nn.ReflectionPad2d(1)\n",
    "\n",
    "        self.C1 = 0.01 ** 2\n",
    "        self.C2 = 0.03 ** 2\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.refl(x)\n",
    "        y = self.refl(y)\n",
    "\n",
    "        mu_x = self.mu_x_pool(x)\n",
    "        mu_y = self.mu_y_pool(y)\n",
    "\n",
    "        sigma_x  = self.sig_x_pool(x ** 2) - mu_x ** 2\n",
    "        sigma_y  = self.sig_y_pool(y ** 2) - mu_y ** 2\n",
    "        sigma_xy = self.sig_xy_pool(x * y) - mu_x * mu_y\n",
    "\n",
    "        SSIM_n = (2 * mu_x * mu_y + self.C1) * (2 * sigma_xy + self.C2)\n",
    "        SSIM_d = (mu_x ** 2 + mu_y ** 2 + self.C1) * (sigma_x + sigma_y + self.C2)\n",
    "\n",
    "        return torch.clamp((1 - SSIM_n / SSIM_d) / 2, 0, 1)a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -i kitti_archives_to_download.txt -P kitti_data/\n",
    "\n",
    "! unzip -o \"kitti_data/*.zip\" -d \"kitti_data/\"\n",
    "\n",
    "# Convert to jpg\n",
    "! sudo apt update\n",
    "! sudo apt install imagemagick\n",
    "! sudo apt install parallel\n",
    "! find kitti_data/ -name '*.png' | parallel 'convert -quality 92 -sampling-factor 2x2,1x1,1x1 {.}.png {.}.jpg && rm {}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a list of all the downloaded files, append $l$ or $r$ corresponding to which one of the cameras in the stereo pair it was taken on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! find . -type f -name '*.jpg' >> all_files.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open('./all_files.txt', 'r')\n",
    "all_lines = fp.readlines()\n",
    "all_lines = [x.strip() for x in all_lines]\n",
    "\n",
    "\n",
    "##########################################################\n",
    "fp2 = open('./eigen_all_files.txt', 'r')\n",
    "eigen_lines = fp2.readlines()\n",
    "eigen_lines = [x.strip() for x in eigen_lines]\n",
    "\n",
    "output_file = open(\"output_files_train.txt\", \"w\")\n",
    "\n",
    "\n",
    "eigen_dict = {}\n",
    "\n",
    "for line in eigen_lines:\n",
    "    filepath, number, RL = line.split(' ')\n",
    "    number = str(int(number))\n",
    "\n",
    "    key = ' '.join([filepath, number])\n",
    "\n",
    "    eigen_dict[key] = RL\n",
    "\n",
    "for i, _ in enumerate(all_lines):\n",
    "    first_part = '/'.join( all_lines[i].split('/')[2:-3] )\n",
    "    second_part = str(int(all_lines[i].split('/')[-1].split('.')[0])) #remove trailing zeros\n",
    "\n",
    "    joined = ' '.join([first_part, second_part])\n",
    "    \n",
    "    if joined in eigen_dict:\n",
    "        output_file.write(' '.join([joined, eigen_dict[joined]]))\n",
    "        output_file.write('\\n')\n",
    "\n",
    "output_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tsVkfx9I77aM",
    "outputId": "eb28bbd8-cd13-444c-d371-e631e7260e2d"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "! uname -a\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRh0z4t5QoiJ"
   },
   "outputs": [],
   "source": [
    "# ! python monodepth2/train.py --model_name stereo_model  --frame_ids 0 --use_stereo --split eigen_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2LaW3qupTEJS"
   },
   "outputs": [],
   "source": [
    "# ! find kitti_data/** -name '*.png' | parallel 'convert -quality 92 -sampling-factor 2x2,1x1,1x1 {.}.png {.}.jpg && rm {}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vuIvPhbqecxe"
   },
   "outputs": [],
   "source": [
    "# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_28_calib.zip\n",
    "# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_28_drive_0001/2011_09_28_drive_0001_sync.zip\n",
    "# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_28_drive_0002/2011_09_28_drive_0002_sync.zip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zXHNpu6ySrUT"
   },
   "outputs": [],
   "source": [
    "# ! rm -rf kitti_data\n",
    "# ! mkdir kitti_data\n",
    "# ! unzip -q 2011_09_28_drive_0001_sync.zip -d kitti_data\n",
    "# ! rm -rf 2011_09_28_drive_0001_sync.zip\n",
    "# ! mv data_temp/2011_09_26/2011_09_26_drive_0095_sync/* kitti_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5JE0YgXjVe-"
   },
   "outputs": [],
   "source": [
    "# ! sudo apt update\n",
    "# ! sudo apt install imagemagick --fix-missing\n",
    "# ! convert -h\n",
    "# ! find kitti_data/ -name '*.png'\n",
    "# ! sudo apt install parallel\n",
    "# convert -quality 92 -sampling-factor 2x2,1x1,1x1 kitti_data/2011_09_26/2011_09_26_drive_0048_sync/image_02/data/0000000005.png jpg && rm {}\n",
    "# ! find kitti_data/2011_09_28 -name '*.png' | parallel 'convert -quality 92 -sampling-factor 2x2,1x1,1x1 {.}.png {.}.jpg && rm {}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5yRZNzAj_gx",
    "outputId": "b94e4d8c-a690-4ead-8d94-0d16e06276ea"
   },
   "outputs": [],
   "source": [
    "! python monodepth2/train.py --model_name S_640x192 --frame_ids 0 --use_stereo --pose_model_type separate_resnet --split eigen_full --data_path /content/kitti_data --num_epochs 10\n",
    "# /content/kitti_data/2011_09_26/2011_09_26_drive_0106_sync/image_02/data/0000000115.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from collections import OrderedDict\n",
    "import layers\n",
    "from layers import *\n",
    "\n",
    "\n",
    "class DepthDecoder(nn.Module):\n",
    "    def __init__(self, num_ch_enc, num_channels_out=1):\n",
    "        super(DepthDecoder, self).__init__()\n",
    "\n",
    "        self.num_channels_out = num_channels_out\n",
    "        self.scales = [0,1,2,3]\n",
    "\n",
    "        self.num_ch_enc = num_ch_enc\n",
    "        self.num_ch_dec = np.array([16, 32, 64, 128, 256])\n",
    "\n",
    "        # decoder   \n",
    "        self.convs = OrderedDict()\n",
    "\n",
    "        ### 4 ###\n",
    "        # upconv_0\n",
    "        num_ch_in = self.num_ch_enc[-1]\n",
    "        num_ch_out = self.num_ch_dec[4]\n",
    "        self.convs[(\"upconv\", 4, 0)] = layers.ConvBlock(num_ch_in, num_ch_out)\n",
    "\n",
    "        # upconv_1\n",
    "        num_ch_in = self.num_ch_dec[4]\n",
    "        #skip connection\n",
    "        num_ch_in += self.num_ch_enc[3]\n",
    "        num_ch_out = self.num_ch_dec[4]\n",
    "        self.convs[(\"upconv\", 4, 1)] = layers.ConvBlock(num_ch_in, num_ch_out)\n",
    "\n",
    "        ### 3 ###\n",
    "        # upconv_0\n",
    "        num_ch_in = self.num_ch_dec[4]\n",
    "        num_ch_out = self.num_ch_dec[3]\n",
    "        self.convs[(\"upconv\", 3, 0)] = layers.ConvBlock(num_ch_in, num_ch_out)\n",
    "\n",
    "        # upconv_1\n",
    "        num_ch_in = self.num_ch_dec[3]\n",
    "        #skip connection\n",
    "        num_ch_in += self.num_ch_enc[2]\n",
    "        num_ch_out = self.num_ch_dec[3]\n",
    "        self.convs[(\"upconv\", 3, 1)] = layers.ConvBlock(num_ch_in, num_ch_out)\n",
    "\n",
    "        ### 2 ###\n",
    "        # upconv_0\n",
    "        num_ch_in = self.num_ch_dec[3]\n",
    "        num_ch_out = self.num_ch_dec[2]\n",
    "        self.convs[(\"upconv\", 2, 0)] = layers.ConvBlock(num_ch_in, num_ch_out)\n",
    "\n",
    "        # upconv_1\n",
    "        num_ch_in = self.num_ch_dec[2]\n",
    "        #skip connection\n",
    "        num_ch_in += self.num_ch_enc[1]\n",
    "        num_ch_out = self.num_ch_dec[2]\n",
    "        self.convs[(\"upconv\", 2, 1)] = layers.ConvBlock(num_ch_in, num_ch_out)\n",
    "\n",
    "        ### 1 ###\n",
    "        # upconv_0\n",
    "        num_ch_in = self.num_ch_dec[2]\n",
    "        num_ch_out = self.num_ch_dec[1]\n",
    "        self.convs[(\"upconv\", 1, 0)] = layers.ConvBlock(num_ch_in, num_ch_out)\n",
    "\n",
    "        # upconv_1\n",
    "        num_ch_in = self.num_ch_dec[1]\n",
    "        #skip connection\n",
    "        num_ch_in += self.num_ch_enc[0]\n",
    "        num_ch_out = self.num_ch_dec[1]\n",
    "        self.convs[(\"upconv\", 1, 1)] = layers.ConvBlock(num_ch_in, num_ch_out)\n",
    "\n",
    "        ### 0 ###\n",
    "        # upconv_0\n",
    "        num_ch_in = self.num_ch_dec[1]\n",
    "        num_ch_out = self.num_ch_dec[0]\n",
    "        self.convs[(\"upconv\", 0, 0)] = layers.ConvBlock(num_ch_in, num_ch_out)\n",
    "\n",
    "        # upconv_1\n",
    "        num_ch_in = self.num_ch_dec[0]\n",
    "        # No skip connection on the last layer\n",
    "        num_ch_out = self.num_ch_dec[0]\n",
    "        self.convs[(\"upconv\", 0, 1)] = layers.ConvBlock(num_ch_in, num_ch_out)\n",
    "\n",
    "\n",
    "\n",
    "        for s in self.scales:\n",
    "            self.convs[(\"dispconv\", s)] = layers.Conv3x3(self.num_ch_dec[s], self.num_channels_out)\n",
    "\n",
    "        self.decoder = nn.ModuleList(list(self.convs.values()))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_features):\n",
    "        self.outputs = {}\n",
    "\n",
    "        x = input_features[-1]\n",
    "\n",
    "        ### 4 ###\n",
    "        x = self.convs[(\"upconv\", 4, 0)](x)\n",
    "        x = [upsample(x)]\n",
    "        x += [input_features[3]]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.convs[(\"upconv\", 4, 1)](x)\n",
    "\n",
    "        ### 3 ###\n",
    "        x = self.convs[(\"upconv\", 3, 0)](x)\n",
    "        x = [upsample(x)]\n",
    "        x += [input_features[2]]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.convs[(\"upconv\", 3, 1)](x)\n",
    "        self.outputs[(\"disp\", 3)] = self.sigmoid(self.convs[(\"dispconv\", 3)](x))\n",
    "\n",
    "        ### 2 ###\n",
    "        x = self.convs[(\"upconv\", 2, 0)](x)\n",
    "        x = [upsample(x)]\n",
    "        x += [input_features[1]]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.convs[(\"upconv\", 2, 1)](x)\n",
    "        self.outputs[(\"disp\", 2)] = self.sigmoid(self.convs[(\"dispconv\", 2)](x))\n",
    "\n",
    "        ### 1 ###\n",
    "        x = self.convs[(\"upconv\", 1, 0)](x)\n",
    "        x = [upsample(x)]\n",
    "        x += [input_features[0]]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.convs[(\"upconv\", 1, 1)](x)\n",
    "        self.outputs[(\"disp\", 1)] = self.sigmoid(self.convs[(\"dispconv\", 1)](x))\n",
    "\n",
    "        ### 0 ###\n",
    "        x = self.convs[(\"upconv\", 0, 0)](x)\n",
    "        x = [upsample(x)]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.convs[(\"upconv\", 0, 1)](x)\n",
    "        self.outputs[(\"disp\", 0)] = self.sigmoid(self.convs[(\"dispconv\", 0)](x))\n",
    "\n",
    "        return self.outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute gradient of image to give Loss function different weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient(t, gauss_blur_sigma=None, kernel_size = 5):\n",
    "  a = torch.Tensor([[1, 0, -1],\n",
    "  [2, 0, -2],\n",
    "  [1, 0, -1]])\n",
    "\n",
    "  a = a.view((1,1,3,3))\n",
    "  G_x = F.conv2d(t, a)\n",
    "\n",
    "  b = torch.Tensor([[1, 2, 1],\n",
    "  [0, 0, 0],\n",
    "  [-1, -2, -1]])\n",
    "\n",
    "  b = b.view((1,1,3,3))\n",
    "  G_y = F.conv2d(t, b)\n",
    "\n",
    "  G = torch.sqrt(torch.pow(G_x,2)+ torch.pow(G_y,2))\n",
    "\n",
    "\n",
    "  if gauss_blur_sigma:\n",
    "    G = kornia.gaussian_blur2d(G, (kernel_size, kernel_size), (gauss_blur_sigma, gauss_blur_sigma))\n",
    "\n",
    "  G -= torch.min(G)\n",
    "  G /= torch.max(G)\n",
    "\n",
    "\n",
    "  return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the gradient function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read in image\n",
    "im = Image.open(\"./kitti_data/2011_09_28/2011_09_28_drive_0002_sync/image_00/data/0000000172.png\")\n",
    "\n",
    "# Convert to tensor and compute gradient\n",
    "t = transforms.ToTensor()\n",
    "t_im = t(im)[None]\n",
    "t_im.shape\n",
    "grad = get_gradient(t_im, gauss_blur_sigma=2)\n",
    "\n",
    "\n",
    "t2 = transforms.ToPILImage()\n",
    "g = t2(grad[0])\n",
    "\n",
    "g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision.models as models\n",
    "\n",
    "def save_model(train):\n",
    "    \"\"\"Save model weights to disk\n",
    "    \"\"\"\n",
    "    save_folder = os.path.join(\"/content/test_model/\")\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    for model_name, model in train.models.items():\n",
    "        save_path = os.path.join(save_folder, \"{}.pth\".format(model_name))\n",
    "        to_save = model.state_dict()\n",
    "        if 'encoder' in model_name:\n",
    "            # save the sizes - these are needed at prediction time\n",
    "            to_save['height'] = train.h\n",
    "            to_save['width'] = train.w\n",
    "            to_save['use_stereo'] = True\n",
    "        torch.save(to_save, save_path)\n",
    "\n",
    "    save_path = os.path.join(save_folder, \"{}.pth\".format(\"adam\"))\n",
    "    torch.save(train.adam_optim.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c7mdtNn2xC6I"
   },
   "outputs": [],
   "source": [
    "#@title Default title text\n",
    "\n",
    "###\n",
    "###\n",
    "### Training code is based on a very-stripped down version of https://arxiv.org/pdf/1806.01260.pdf\n",
    "### Written only with stereo based training with reduced feature set\n",
    "###\n",
    "###\n",
    "\n",
    "\n",
    "import monodepth2.layers, monodepth2.utils, monodepth2.trainer\n",
    "import monodepth2.networks\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import timeit\n",
    "import PIL.Image as pil\n",
    "\n",
    "\n",
    "class MyTraining(object):\n",
    "    def __init__(self, batch, epoch):\n",
    "        self.batch = batch\n",
    "        self.epoch_max = epoch\n",
    "        self.h = 192\n",
    "        self.w = 640\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "        # For listing which frames to check\n",
    "        # For simple stereo is only current (0) and estimated pair (s)\n",
    "        self.frames = [0, 's']\n",
    "\n",
    "        # For multi-scale estimation\n",
    "        self.loss_scales = [0, 1, 2, 3]\n",
    "\n",
    "        # Weighting for loss function\n",
    "        self.alpha = 0.85\n",
    "\n",
    "        # Define projections for pose estimation\n",
    "        self.depth_projection = {}\n",
    "        self.projection_3d = {}\n",
    "        for loss_scale in self.loss_scales:\n",
    "            h = int(self.h / (2 ** loss_scale))\n",
    "            w = int(self.w / (2 ** loss_scale))\n",
    "\n",
    "            self.depth_projection[loss_scale] = monodepth2.layers.BackprojectDepth(self.batch, h, w).to(self.device)\n",
    "            self.projection_3d[loss_scale] = monodepth2.layers.Project3D(self.batch, h, w).to(self.device)\n",
    "        \n",
    "        ## Declare Depth Network\n",
    "        self.depth_encoder_network = monodepth2.networks.ResnetEncoder(18, True).to(self.device)\n",
    "        self.depth_decoder_network = monodepth2.networks.DepthDecoder(self.depth_encoder_network.num_ch_enc, \n",
    "                                                [0, 1, 2, 3]).to(self.device) \n",
    "\n",
    "        ## Declare Pose Network\n",
    "        self.pose_encoder_network = monodepth2.networks.ResnetEncoder(18, True, num_input_images=2).to(self.device)\n",
    "        self.pose_decoder_network = monodepth2.networks.PoseDecoder(self.pose_encoder_network.num_ch_enc, \n",
    "                                                num_input_features=1, num_frames_to_predict_for=2).to(self.device)\n",
    "\n",
    "        self.models = {\"encoder\": self.depth_encoder_network, \"depth\": self.depth_decoder_network,\n",
    "                       \"pose_encoder\": self.pose_encoder_network, \"pose\": self.pose_decoder_network}\n",
    "\n",
    "        # Set up our excerpt of Kitti Dataset\n",
    "        training_files = monodepth2.utils.readlines(\"/content/monodepth2/splits/eigen_full/train_files.txt\")\n",
    "        validation_files = monodepth2.utils.readlines(\"/content/monodepth2/splits/eigen_full/val_files.txt\")\n",
    "\n",
    "        train_set = monodepth2.datasets.kitti_dataset.KITTIRAWDataset(\"/content/kitti_data\", training_files, self.h, self.w, self.frames, 4, is_train=True, img_ext='.jpg')\n",
    "        self.train_loader = DataLoader(train_set, self.batch, True, num_workers=6, pin_memory=True, drop_last=True)\n",
    "        val_set = monodepth2.datasets.kitti_dataset.KITTIRAWDataset(\"/content/kitti_data\", validation_files, self.h, self.w, self.frames, 4, is_train=False, img_ext='.jpg')\n",
    "        self.val_loader = DataLoader(val_set, self.batch, True, num_workers=6, pin_memory=True, drop_last=True)\n",
    "\n",
    "    def run_training_loop(self):\n",
    "        self.all_params = list(self.depth_encoder_network.parameters())\n",
    "        self.all_params += list(self.depth_decoder_network.parameters())\n",
    "        self.all_params += list(self.pose_encoder_network.parameters())\n",
    "        self.all_params += list(self.pose_decoder_network.parameters())\n",
    "\n",
    "        # Same learning rate configuration as https://arxiv.org/pdf/1806.01260.pdf\n",
    "        self.adam_optim = optim.Adam(self.all_params, 1e-4)\n",
    "        self.lr_sched = optim.lr_scheduler.StepLR(self.adam_optim, 15, 0.1)\n",
    "        self.ssim_loss_func = monodepth2.layers.SSIM().to(self.device)\n",
    "\n",
    "        self.epoch_count = 0\n",
    "        self.losses  = []\n",
    "\n",
    "        for self.epoch_count in range(self.epoch_max):\n",
    "            # Per Epoch\n",
    "            self.lr_sched.step()\n",
    "            self.depth_encoder_network.train()\n",
    "            self.depth_decoder_network.train()\n",
    "            self.pose_encoder_network.train()\n",
    "            self.pose_decoder_network.train()\n",
    "\n",
    "            for idx, inputs in enumerate(self.train_loader):\n",
    "                # Push to GPU\n",
    "                for key, ipt in inputs.items():\n",
    "                    inputs[key] = ipt.to(self.device)\n",
    "\n",
    "                # Per Batch Code\n",
    "                batch_start_time = timeit.default_timer()\n",
    "\n",
    "                feature_identifications = self.depth_encoder_network(inputs[\"color_aug\", 0, 0])\n",
    "                outputs = self.depth_decoder_network(feature_identifications)\n",
    "\n",
    "                self.estimate_stereo_predictions(inputs, outputs)\n",
    "                loss = self.batch_loss_func(inputs, outputs)\n",
    "\n",
    "                self.adam_optim.zero_grad()\n",
    "                loss[\"loss\"].backward()\n",
    "                self.losses.append(loss[\"loss\"].item())\n",
    "                self.adam_optim.step()\n",
    "\n",
    "                batch_duration = timeit.default_timer() - batch_start_time\n",
    "\n",
    "                # Do something with batch duration and losses\n",
    "            \n",
    "            print(\"Finished Epoch: {} with Loss {}\".format(self.epoch_count, self.losses[-1]))\n",
    "\n",
    "    def estimate_stereo_predictions(self, inputs, outputs):\n",
    "        # Generate estimated stereo pair using the pose networks\n",
    "        for loss_scale in self.loss_scales:\n",
    "            estimated_disparity = outputs[(\"disp\", loss_scale)]\n",
    "            disp = F.interpolate(estimated_disparity, [self.h, self.w], mode=\"bilinear\", align_corners=False)\n",
    "            base = 0 # base scale\n",
    "\n",
    "            # Convert sigmoid disparity to depth estimate\n",
    "            scaled_disp = 0.001 + (10 - 0.001) * disp\n",
    "            depth = 1 / scaled_disp\n",
    "            outputs[(\"depth\", 0, loss_scale)] = depth\n",
    "\n",
    "            # Finalize Stereo Estimates\n",
    "            # Fetch camera extrinsics generated by KITTIRAWDataset\n",
    "            extrinsics = inputs[\"stereo_T\"]\n",
    "\n",
    "            camera_coords = self.depth_projection[base](depth, inputs[(\"inv_K\", base)])\n",
    "            pixel_coords = self.projection_3d[base](camera_coords, inputs[(\"K\", base)], extrinsics)\n",
    "\n",
    "            outputs[(\"sample\", 's', loss_scale)] = pixel_coords\n",
    "            outputs[(\"color\", 's', loss_scale)] = F.grid_sample(\n",
    "                inputs[(\"color\", 's', base)], outputs[(\"sample\", 's', loss_scale)], padding_mode=\"border\")\n",
    "                \n",
    "    # Compute SSIM and L1 Loss\n",
    "    def reprojection(self, prediction, target):\n",
    "        ssim_loss = self.ssim_loss_func(prediction, target).mean(1, True)\n",
    "        reproj_losses = self.alpha*ssim_loss + (1-self.alpha)*(torch.abs(target-prediction).mean(1, True))\n",
    "        return reproj_losses\n",
    "\n",
    "    def batch_loss_func(self, inputs, outputs):\n",
    "        batch_loss = {}\n",
    "        complete_losses = 0\n",
    "\n",
    "        for loss_scale in self.loss_scales:\n",
    "            reproj_losses = []\n",
    "\n",
    "            reproj_losses.append(self.reprojection(outputs[(\"color\", 's', loss_scale)], inputs[(\"color\", 0, 0)]))\n",
    "            reproj_losses = torch.cat(reproj_losses, 1)\n",
    "\n",
    "            identity_loss = []\n",
    "            identity_loss.append(self.reprojection(inputs[(\"color\", 's', 0)], inputs[(\"color\", 0, 0)]))\n",
    "            identity_loss = torch.cat(identity_loss, 1)\n",
    "\n",
    "\n",
    "            # Add some minor noise to ensure no repeated values\n",
    "            identity_loss += torch.rand(identity_loss.shape).cuda() * 0.00001\n",
    "            total = torch.cat((identity_loss, reproj_losses), dim=1)\n",
    "            val, idxs = torch.min(total, dim=1)\n",
    "            outputs[\"identity_selection/{}\".format(loss_scale)] = (idxs > identity_loss.shape[1] - 1).float()\n",
    "\n",
    "            current_loss = val.mean()\n",
    "\n",
    "            mean_disp = outputs[(\"disp\", loss_scale)].mean(2, True).mean(3, True)\n",
    "            norm_disp = outputs[(\"disp\", loss_scale)] / (mean_disp + 1e-7)\n",
    "            smooth_loss = monodepth2.layers.get_smooth_loss(norm_disp, inputs[(\"color\", 0, loss_scale)])\n",
    "\n",
    "            current_loss += 1e-3 * smooth_loss / (2 ** loss_scale)\n",
    "            complete_losses += current_loss\n",
    "            batch_loss[\"loss/{}\".format(loss_scale)] = current_loss\n",
    "\n",
    "        complete_losses /= len(self.loss_scales)\n",
    "        batch_loss[\"loss\"] = complete_losses\n",
    "        return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "ow5DP4v9zQEH",
    "outputId": "fd2a11a7-5a91-42ab-e8fc-60d42c667986"
   },
   "outputs": [],
   "source": [
    "train = MyTraining(24, 10)\n",
    "train.run_training_loop()\n",
    "save_model(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test on car image\n",
    "! python monodepth2/test_simple.py --image_path monodepth2/assets/test_image.jpg --model_name mono+stereo_640x192 --model_path /content/test_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# my_losses = np.copy(train.losses)\n",
    "\n",
    "plt.plot(range(len(my_losses)), my_losses, 'b-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python monodepth2/export_gt_depth.py --data_path kitti_data --split eigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python monodepth2/evaluate_depth.py --data_path kitti_data --load_weights_folder /content/test_model/ --eval_stereo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVMqZmbOPPeE"
   },
   "source": [
    "# References\n",
    "\n",
    "[1] https://arxiv.org/pdf/1806.01260.pdf\n",
    "\n",
    "[2] https://arxiv.org/pdf/1511.08861.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOe4WkHQlDu4yVyzJ7V8+TL",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1maJgn9Dp7KSMr0Vla80dQFTvxXxRJOLj",
   "name": "cs484_project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
