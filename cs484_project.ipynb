{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cs484_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1maJgn9Dp7KSMr0Vla80dQFTvxXxRJOLj",
      "authorship_tag": "ABX9TyMbwzdHiNoIvenlUyLtgzHJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArjunNarayan2066/CS484_project/blob/fixed_training_loop/cs484_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnRlwbQ9z4kL",
        "outputId": "e364dec7-b1da-46b1-af2d-4fe44db73112"
      },
      "source": [
        "# Install dependencies\r\n",
        "! nvcc --version\r\n",
        "\r\n",
        "! pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html\r\n",
        "! pip install tensorboardX==1.4\r\n",
        "! pip install opencv-python==3.3.1.11\r\n",
        "\r\n",
        "# Clone repo\r\n",
        "! git clone https://github.com/ArjunNarayan2066/monodepth2.git"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.7.1+cu101 in /usr/local/lib/python3.6/dist-packages (1.7.1+cu101)\n",
            "Requirement already satisfied: torchvision==0.8.2+cu101 in /usr/local/lib/python3.6/dist-packages (0.8.2+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (1.19.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.8.2+cu101) (7.0.0)\n",
            "Requirement already satisfied: tensorboardX==1.4 in /usr/local/lib/python3.6/dist-packages (1.4)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.4) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.4) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.4) (1.19.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX==1.4) (50.3.2)\n",
            "Requirement already satisfied: opencv-python==3.3.1.11 in /usr/local/lib/python3.6/dist-packages (3.3.1.11)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python==3.3.1.11) (1.19.4)\n",
            "Cloning into 'monodepth2'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 164 (delta 2), reused 7 (delta 2), pack-reused 150\u001b[K\n",
            "Receiving objects: 100% (164/164), 10.27 MiB | 39.10 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsVkfx9I77aM",
        "outputId": "55a6be47-7ea2-4fac-8683-0cff5a29d24a"
      },
      "source": [
        "import torch\r\n",
        "! uname -a\r\n",
        "print(torch.cuda.is_available())\r\n",
        "print(torch.cuda.get_device_name())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linux 28dd3f85ea1d 4.19.112+ #1 SMP Thu Jul 23 08:00:38 PDT 2020 x86_64 x86_64 x86_64 GNU/Linux\n",
            "True\n",
            "Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRh0z4t5QoiJ"
      },
      "source": [
        "# ! python monodepth2/train.py --model_name stereo_model  --frame_ids 0 --use_stereo --split eigen_full"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuIvPhbqecxe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6021e541-e644-4af5-f50f-fad9258a3803"
      },
      "source": [
        "! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_28_calib.zip\r\n",
        "! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_28_drive_0001/2011_09_28_drive_0001_sync.zip\r\n",
        "! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_28_drive_0002/2011_09_28_drive_0002_sync.zip\r\n",
        "\r\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-18 04:00:17--  https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_28_calib.zip\n",
            "Resolving s3.eu-central-1.amazonaws.com (s3.eu-central-1.amazonaws.com)... 52.219.74.199\n",
            "Connecting to s3.eu-central-1.amazonaws.com (s3.eu-central-1.amazonaws.com)|52.219.74.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4073 (4.0K) [application/zip]\n",
            "Saving to: ‘2011_09_28_calib.zip’\n",
            "\n",
            "2011_09_28_calib.zi 100%[===================>]   3.98K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-12-18 04:00:18 (182 MB/s) - ‘2011_09_28_calib.zip’ saved [4073/4073]\n",
            "\n",
            "--2020-12-18 04:00:18--  https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_28_drive_0001/2011_09_28_drive_0001_sync.zip\n",
            "Resolving s3.eu-central-1.amazonaws.com (s3.eu-central-1.amazonaws.com)... 52.219.74.199\n",
            "Connecting to s3.eu-central-1.amazonaws.com (s3.eu-central-1.amazonaws.com)|52.219.74.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 424412944 (405M) [application/zip]\n",
            "Saving to: ‘2011_09_28_drive_0001_sync.zip’\n",
            "\n",
            "2011_09_28_drive_00 100%[===================>] 404.75M  2.78MB/s    in 2m 2s   \n",
            "\n",
            "2020-12-18 04:02:21 (3.32 MB/s) - ‘2011_09_28_drive_0001_sync.zip’ saved [424412944/424412944]\n",
            "\n",
            "--2020-12-18 04:02:21--  https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_28_drive_0002/2011_09_28_drive_0002_sync.zip\n",
            "Resolving s3.eu-central-1.amazonaws.com (s3.eu-central-1.amazonaws.com)... 52.219.72.139\n",
            "Connecting to s3.eu-central-1.amazonaws.com (s3.eu-central-1.amazonaws.com)|52.219.72.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1458258535 (1.4G) [application/zip]\n",
            "Saving to: ‘2011_09_28_drive_0002_sync.zip’\n",
            "\n",
            "2011_09_28_drive_00 100%[===================>]   1.36G  18.8MB/s    in 84s     \n",
            "\n",
            "2020-12-18 04:03:45 (16.6 MB/s) - ‘2011_09_28_drive_0002_sync.zip’ saved [1458258535/1458258535]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXHNpu6ySrUT"
      },
      "source": [
        "! mkdir kitti_data\r\n",
        "! unzip -q 2011_09_28_drive_0001_sync.zip -d kitti_data\r\n",
        "! rm -rf 2011_09_28_drive_0001_sync.zip\r\n",
        "! unzip -q 2011_09_28_drive_0002_sync.zip -d kitti_data\r\n",
        "! rm -rf 2011_09_28_drive_0002_sync.zip\r\n",
        "! unzip -q 2011_09_28_calib.zip -d kitti_data\r\n",
        "! rm -rf 2011_09_28_calib.zip"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5JE0YgXjVe-"
      },
      "source": [
        "! sudo apt update\r\n",
        "! sudo apt install imagemagick\r\n",
        "! sudo apt install parallel\r\n",
        "! find kitti_data/2011_09_28 -name '*.png' | parallel 'convert -quality 92 -sampling-factor 2x2,1x1,1x1 {.}.png {.}.jpg && rm {}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2l-qT82snuo"
      },
      "source": [
        "# ! python monodepth2/export_gt_depth.py --data_path kitti_data --split eigen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWvl5VeiuppM"
      },
      "source": [
        "# ! python monodepth2/evaluate_depth.py --data_path kitti_data --load_weights_folder /root/tmp/S_640x192/models/weights_19/ --eval_stereo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgYHittywMt5"
      },
      "source": [
        "\r\n",
        "import os\r\n",
        "import torchvision.models as models\r\n",
        "\r\n",
        "def save_model(train):\r\n",
        "    \"\"\"Save model weights to disk\r\n",
        "    \"\"\"\r\n",
        "    save_folder = os.path.join(\"/content/test_model/\")\r\n",
        "    if not os.path.exists(save_folder):\r\n",
        "        os.makedirs(save_folder)\r\n",
        "\r\n",
        "    for model_name, model in train.models.items():\r\n",
        "        save_path = os.path.join(save_folder, \"{}.pth\".format(model_name))\r\n",
        "        to_save = model.state_dict()\r\n",
        "        if 'encoder' in model_name:\r\n",
        "            # save the sizes - these are needed at prediction time\r\n",
        "            to_save['height'] = train.h\r\n",
        "            to_save['width'] = train.w\r\n",
        "            to_save['use_stereo'] = True\r\n",
        "        torch.save(to_save, save_path)\r\n",
        "\r\n",
        "    save_path = os.path.join(save_folder, \"{}.pth\".format(\"adam\"))\r\n",
        "    torch.save(train.adam_optim.state_dict(), save_path)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "c7mdtNn2xC6I"
      },
      "source": [
        "#@title Default title text\n",
        "\n",
        "###\n",
        "###\n",
        "### Training code is based on a very-stripped down version of https://arxiv.org/pdf/1806.01260.pdf\n",
        "### Written only with stereo based training with reduced feature set\n",
        "###\n",
        "###\n",
        "\n",
        "\n",
        "import monodepth2.layers, monodepth2.utils, monodepth2.trainer\n",
        "import monodepth2.networks\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import timeit\n",
        "import PIL.Image as pil\n",
        "\n",
        "\n",
        "class MyTraining(object):\n",
        "    def __init__(self, batch):\n",
        "        self.batch = batch\n",
        "        self.h = 192\n",
        "        self.w = 640\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.device(\"cuda\")\n",
        "        else:\n",
        "            self.device = torch.device(\"cpu\")\n",
        "\n",
        "        # For listing which frames to check\n",
        "        # For simple stereo is only current (0) and estimated pair (s)\n",
        "        self.frames = [0, 's']\n",
        "\n",
        "        # For multi-scale estimation\n",
        "        self.loss_scales = [0, 1, 2, 3]\n",
        "\n",
        "        # Weighting for loss function\n",
        "        self.alpha = 0.85\n",
        "\n",
        "        # Define projections for pose estimation\n",
        "        self.depth_projection = {}\n",
        "        self.projection_3d = {}\n",
        "        for loss_scale in self.loss_scales:\n",
        "            h = int(self.h / (2 ** loss_scale))\n",
        "            w = int(self.w / (2 ** loss_scale))\n",
        "\n",
        "            self.depth_projection[loss_scale] = monodepth2.layers.BackprojectDepth(self.batch, h, w).to(self.device)\n",
        "            self.projection_3d[loss_scale] = monodepth2.layers.Project3D(self.batch, h, w).to(self.device)\n",
        "        \n",
        "        ## Declare Depth Network\n",
        "        self.depth_encoder_network = monodepth2.networks.ResnetEncoder(18, True).to(self.device)\n",
        "        self.depth_decoder_network = monodepth2.networks.DepthDecoder(self.depth_encoder_network.num_ch_enc, \n",
        "                                                [0, 1, 2, 3]).to(self.device) \n",
        "\n",
        "        ## Declare Pose Network\n",
        "        self.pose_encoder_network = monodepth2.networks.ResnetEncoder(18, True, num_input_images=2).to(self.device)\n",
        "        self.pose_decoder_network = monodepth2.networks.PoseDecoder(self.pose_encoder_network.num_ch_enc, \n",
        "                                                num_input_features=1, num_frames_to_predict_for=2).to(self.device)\n",
        "\n",
        "        self.models = {\"encoder\": self.depth_encoder_network, \"depth\": self.depth_decoder_network,\n",
        "                       \"pose_encoder\": self.pose_encoder_network, \"pose\": self.pose_decoder_network}\n",
        "\n",
        "        # Set up our excerpt of Kitti Dataset\n",
        "        training_files = monodepth2.utils.readlines(\"/content/monodepth2/splits/eigen_full/train_files.txt\")\n",
        "        validation_files = monodepth2.utils.readlines(\"/content/monodepth2/splits/eigen_full/val_files.txt\")\n",
        "\n",
        "        train_set = monodepth2.datasets.kitti_dataset.KITTIRAWDataset(\"/content/kitti_data\", training_files, self.h, self.w, self.frames, 4, is_train=True, img_ext='.jpg')\n",
        "        self.train_loader = DataLoader(train_set, self.batch, True, num_workers=6, pin_memory=True, drop_last=True)\n",
        "        val_set = monodepth2.datasets.kitti_dataset.KITTIRAWDataset(\"/content/kitti_data\", validation_files, self.h, self.w, self.frames, 4, is_train=False, img_ext='.jpg')\n",
        "        self.val_loader = DataLoader(val_set, self.batch, True, num_workers=6, pin_memory=True, drop_last=True)\n",
        "\n",
        "    def run_training_loop(self):\n",
        "        self.all_params = list(self.depth_encoder_network.parameters())\n",
        "        self.all_params += list(self.depth_decoder_network.parameters())\n",
        "        self.all_params += list(self.pose_encoder_network.parameters())\n",
        "        self.all_params += list(self.pose_decoder_network.parameters())\n",
        "\n",
        "        # Same learning rate configuration as https://arxiv.org/pdf/1806.01260.pdf\n",
        "        self.adam_optim = optim.Adam(self.all_params, 1e-4)\n",
        "        self.lr_sched = optim.lr_scheduler.StepLR(self.adam_optim, 15, 0.1)\n",
        "        self.ssim_loss_func = monodepth2.layers.SSIM().to(self.device)\n",
        "\n",
        "        self.epoch_count = 0\n",
        "        self.losses  = []\n",
        "\n",
        "        for self.epoch_count in range(5):\n",
        "            # Per Epoch\n",
        "            self.lr_sched.step()\n",
        "            self.depth_encoder_network.train()\n",
        "            self.depth_decoder_network.train()\n",
        "            self.pose_encoder_network.train()\n",
        "            self.pose_decoder_network.train()\n",
        "\n",
        "            for idx, inputs in enumerate(self.train_loader):\n",
        "                # Push to GPU\n",
        "                for key, ipt in inputs.items():\n",
        "                    inputs[key] = ipt.to(self.device)\n",
        "\n",
        "                # Per Batch Code\n",
        "                batch_start_time = timeit.default_timer()\n",
        "\n",
        "                feature_identifications = self.depth_encoder_network(inputs[\"color_aug\", 0, 0])\n",
        "                outputs = self.depth_decoder_network(feature_identifications)\n",
        "\n",
        "                self.estimate_stereo_predictions(inputs, outputs)\n",
        "                loss = self.batch_loss_func(inputs, outputs)\n",
        "\n",
        "                self.adam_optim.zero_grad()\n",
        "                loss[\"loss\"].backward()\n",
        "                self.losses.append(loss[\"loss\"].item())\n",
        "                self.adam_optim.step()\n",
        "\n",
        "                batch_duration = timeit.default_timer() - batch_start_time\n",
        "\n",
        "                # Do something with batch duration and losses\n",
        "            \n",
        "            print(\"Finished Epoch: {} with Loss {}\".format(self.epoch_count, self.losses[-1]))\n",
        "\n",
        "    def estimate_stereo_predictions(self, inputs, outputs):\n",
        "        # Generate estimated stereo pair using the pose networks\n",
        "        for loss_scale in self.loss_scales:\n",
        "            estimated_disparity = outputs[(\"disp\", loss_scale)]\n",
        "            disp = F.interpolate(estimated_disparity, [self.h, self.w], mode=\"bilinear\", align_corners=False)\n",
        "            base = 0 # base scale\n",
        "\n",
        "            # Convert sigmoid disparity to depth estimate\n",
        "            scaled_disp = 0.001 + (10 - 0.001) * disp\n",
        "            depth = 1 / scaled_disp\n",
        "            outputs[(\"depth\", 0, loss_scale)] = depth\n",
        "\n",
        "            # Finalize Stereo Estimates\n",
        "            # Fetch camera extrinsics generated by KITTIRAWDataset\n",
        "            extrinsics = inputs[\"stereo_T\"]\n",
        "\n",
        "            camera_coords = self.depth_projection[base](depth, inputs[(\"inv_K\", base)])\n",
        "            pixel_coords = self.projection_3d[base](camera_coords, inputs[(\"K\", base)], extrinsics)\n",
        "\n",
        "            outputs[(\"sample\", 's', loss_scale)] = pixel_coords\n",
        "            outputs[(\"color\", 's', loss_scale)] = F.grid_sample(\n",
        "                inputs[(\"color\", 's', base)], outputs[(\"sample\", 's', loss_scale)], padding_mode=\"border\")\n",
        "                \n",
        "    def reprojection(self, prediction, target):\n",
        "        ssim_loss = self.ssim_loss_func(prediction, target).mean(1, True)\n",
        "        reproj_losses = self.alpha*ssim_loss + (1-self.alpha)*(torch.abs(target-prediction).mean(1, True))\n",
        "        return reproj_losses\n",
        "\n",
        "    def batch_loss_func(self, inputs, outputs):\n",
        "        batch_loss = {}\n",
        "        complete_losses = 0\n",
        "\n",
        "        for loss_scale in self.loss_scales:\n",
        "            reproj_losses = []\n",
        "\n",
        "            reproj_losses.append(self.reprojection(outputs[(\"color\", 's', loss_scale)], inputs[(\"color\", 0, 0)]))\n",
        "            reproj_losses = torch.cat(reproj_losses, 1)\n",
        "\n",
        "            identity_loss = []\n",
        "            identity_loss.append(self.reprojection(inputs[(\"color\", 's', 0)], inputs[(\"color\", 0, 0)]))\n",
        "            identity_loss = torch.cat(identity_loss, 1)\n",
        "\n",
        "\n",
        "            # Add some minor noise to ensure no repeated values\n",
        "            identity_loss += torch.rand(identity_loss.shape).cuda() * 0.00001\n",
        "            total = torch.cat((identity_loss, reproj_losses), dim=1)\n",
        "            val, idxs = torch.min(total, dim=1)\n",
        "            outputs[\"identity_selection/{}\".format(loss_scale)] = (idxs > identity_loss.shape[1] - 1).float()\n",
        "\n",
        "            current_loss = val.mean()\n",
        "\n",
        "            mean_disp = outputs[(\"disp\", loss_scale)].mean(2, True).mean(3, True)\n",
        "            norm_disp = outputs[(\"disp\", loss_scale)] / (mean_disp + 1e-7)\n",
        "            smooth_loss = monodepth2.layers.get_smooth_loss(norm_disp, inputs[(\"color\", 0, loss_scale)])\n",
        "\n",
        "            current_loss += 1e-3 * smooth_loss / (2 ** loss_scale)\n",
        "            complete_losses += current_loss\n",
        "            batch_loss[\"loss/{}\".format(loss_scale)] = current_loss\n",
        "\n",
        "        complete_losses /= len(self.loss_scales)\n",
        "        batch_loss[\"loss\"] = complete_losses\n",
        "        return batch_loss\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ow5DP4v9zQEH",
        "outputId": "f9ca4531-1192-422f-80ee-e5c5ccb109ca"
      },
      "source": [
        "train = MyTraining(20)\r\n",
        "train.run_training_loop()\r\n",
        "save_model(train)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3385: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Epoch: 0 with Loss 0.14371290802955627\n",
            "Finished Epoch: 1 with Loss 0.1369514912366867\n",
            "Finished Epoch: 2 with Loss 0.1367199420928955\n",
            "Finished Epoch: 3 with Loss 0.11680222302675247\n",
            "Finished Epoch: 4 with Loss 0.12205266207456589\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVMqZmbOPPeE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98ed921e-7c3f-4322-c5ec-18177d4ed0df"
      },
      "source": [
        "# Simple test on car image\r\n",
        "! python monodepth2/test_simple.py --image_path monodepth2/assets/test_image.jpg --model_name mono+stereo_640x192 --model_path /content/test_model/"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-> Loading model from  /content/test_model/\n",
            "   Loading pretrained encoder\n",
            "   Loading pretrained decoder\n",
            "-> Predicting on 1 test images\n",
            "   Processed 1 of 1 images - saved prediction to monodepth2/assets/test_image_disp.jpeg\n",
            "-> Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "geC8lerkQSox",
        "outputId": "944ec558-8eb5-4d57-eec2-030fc6c34ae6"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "plt.plot(range(len(train.losses)), train.losses, 'b-')\r\n",
        "plt.show()\r\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e329aef8ec85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'MyTraining' object has no attribute 'losses'"
          ]
        }
      ]
    }
  ]
}