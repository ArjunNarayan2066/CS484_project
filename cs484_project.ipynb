{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cs484_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1maJgn9Dp7KSMr0Vla80dQFTvxXxRJOLj",
      "authorship_tag": "ABX9TyPpNqfzgjYYjPlxxSLDwriH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArjunNarayan2066/CS484_project/blob/fixed_training_loop/cs484_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnRlwbQ9z4kL",
        "outputId": "06c378c2-61af-4ca7-baba-d3beaef6e3f6"
      },
      "source": [
        "# Install dependencies\r\n",
        "! nvcc --version\r\n",
        "\r\n",
        "! pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html\r\n",
        "! pip install tensorboardX==1.4\r\n",
        "! pip install opencv-python==3.3.1.11\r\n",
        "\r\n",
        "# Clone repo\r\n",
        "! git clone https://github.com/nianticlabs/monodepth2.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.7.1+cu101 in /usr/local/lib/python3.6/dist-packages (1.7.1+cu101)\n",
            "Requirement already satisfied: torchvision==0.8.2+cu101 in /usr/local/lib/python3.6/dist-packages (0.8.2+cu101)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.8.2+cu101) (7.0.0)\n",
            "Requirement already satisfied: tensorboardX==1.4 in /usr/local/lib/python3.6/dist-packages (1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.4) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.4) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.4) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX==1.4) (50.3.2)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3021, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/cli/base_command.py\", line 153, in _main\n",
            "    status = self.run(options, args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py\", line 438, in run\n",
            "    self._warn_about_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py\", line 568, in _warn_about_conflicts\n",
            "    package_set, _dep_info = check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/operations/check.py\", line 114, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/operations/check.py\", line 53, in create_package_set_from_installed\n",
            "    package_set[name] = PackageDetails(dist.version, dist.requires())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2736, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3023, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3033, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3094, in parse_requirements\n",
            "    yield Requirement(line)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3101, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/packaging/requirements.py\", line 93, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 1929, in parseString\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 1669, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 4037, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 1669, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 4222, in parseImpl\n",
            "    ret = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 1669, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 4037, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 1669, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 4817, in parseImpl\n",
            "    loc, tokens = self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 1669, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 4037, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 1702, in _parseNoCache\n",
            "    tokens = fn(instring, tokensStart, retTokens)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 1308, in wrapper\n",
            "    ret = func(*args[limit[0]:])\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/packaging/requirements.py\", line 62, in <lambda>\n",
            "    lambda s, l, t: Marker(s[t._original_start : t._original_end])\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/packaging/markers.py\", line 270, in __init__\n",
            "    self._markers = _coerce_parse_result(MARKER.parseString(marker))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 1929, in parseString\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 1669, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 4037, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 1669, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 4430, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 1669, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 4020, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 1669, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 4222, in parseImpl\n",
            "    ret = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 1669, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 4430, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 1669, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 4037, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 1669, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 4222, in parseImpl\n",
            "    ret = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 1669, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 4222, in parseImpl\n",
            "    ret = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 1673, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 2871, in parseImpl\n",
            "    raise ParseException(instring, loc, self.errmsg, self)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 302, in __init__\n",
            "    self.msg = msg\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/main.py\", line 47, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/cli/base_command.py\", line 103, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/cli/base_command.py\", line 183, in _main\n",
            "    logger.debug('Exception information:', exc_info=True)\n",
            "  File \"/usr/lib/python3.6/logging/__init__.py\", line 1296, in debug\n",
            "    self._log(DEBUG, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.6/logging/__init__.py\", line 1444, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.6/logging/__init__.py\", line 1454, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.6/logging/__init__.py\", line 1516, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.6/logging/__init__.py\", line 865, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.6/logging/handlers.py\", line 73, in emit\n",
            "    logging.FileHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.6/logging/__init__.py\", line 1072, in emit\n",
            "    StreamHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.6/logging/__init__.py\", line 994, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.6/logging/__init__.py\", line 840, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/utils/logging.py\", line 151, in format\n",
            "    formatted = super(IndentingFormatter, self).format(record)\n",
            "  File \"/usr/lib/python3.6/logging/__init__.py\", line 585, in format\n",
            "    record.exc_text = self.formatException(record.exc_info)\n",
            "  File \"/usr/lib/python3.6/logging/__init__.py\", line 535, in formatException\n",
            "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
            "  File \"/usr/lib/python3.6/traceback.py\", line 104, in print_exception\n",
            "    type(value), value, tb, limit=limit).format(chain=chain):\n",
            "  File \"/usr/lib/python3.6/traceback.py\", line 509, in __init__\n",
            "    capture_locals=capture_locals)\n",
            "  File \"/usr/lib/python3.6/traceback.py\", line 364, in extract\n",
            "    f.line\n",
            "  File \"/usr/lib/python3.6/traceback.py\", line 286, in line\n",
            "    self._line = linecache.getline(self.filename, self.lineno).strip()\n",
            "  File \"/usr/lib/python3.6/linecache.py\", line 16, in getline\n",
            "    lines = getlines(filename, module_globals)\n",
            "  File \"/usr/lib/python3.6/linecache.py\", line 47, in getlines\n",
            "    return updatecache(filename, module_globals)\n",
            "  File \"/usr/lib/python3.6/linecache.py\", line 136, in updatecache\n",
            "    with tokenize.open(fullname) as fp:\n",
            "  File \"/usr/lib/python3.6/tokenize.py\", line 454, in open\n",
            "    encoding, lines = detect_encoding(buffer.readline)\n",
            "  File \"/usr/lib/python3.6/tokenize.py\", line 423, in detect_encoding\n",
            "    first = read_or_stop()\n",
            "  File \"/usr/lib/python3.6/tokenize.py\", line 381, in read_or_stop\n",
            "    return readline()\n",
            "KeyboardInterrupt\n",
            "Requirement already satisfied: opencv-python==3.3.1.11 in /usr/local/lib/python3.6/dist-packages (3.3.1.11)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python==3.3.1.11) (1.18.5)\n",
            "fatal: destination path 'monodepth2' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AK6si-eI1Owv"
      },
      "source": [
        "# ! python monodepth2/test_simple.py --image_path monodepth2/assets/test_image.jpg --model_name mono+stereo_640x192 --model_path /root/tmp/S_640x192/models/weights_19\r\n",
        "# /root/tmp/S_640x192/models/weights_9/encoder.pth"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyVajA6BA8CM"
      },
      "source": [
        "# ! rm -rf *.zip\r\n",
        "# ! rm -rf kitti_data\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "7mwHPQeT6dsV",
        "outputId": "d8dbf79f-30f9-471e-ff3e-6484657a63ae"
      },
      "source": [
        "\"\"\"\r\n",
        "! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0001/2011_09_26_drive_0001_sync.zip\r\n",
        "! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0002/2011_09_26_drive_0002_sync.zip\r\n",
        "! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0005/2011_09_26_drive_0005_sync.zip\r\n",
        "! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0009/2011_09_26_drive_0009_sync.zip\r\n",
        "! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0011/2011_09_26_drive_0011_sync.zip\r\n",
        "! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0013/2011_09_26_drive_0013_sync.zip\r\n",
        "# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0014/2011_09_26_drive_0014_sync.zip\r\n",
        "! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0015/2011_09_26_drive_0015_sync.zip\r\n",
        "! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0017/2011_09_26_drive_0017_sync.zip\r\n",
        "# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0018/2011_09_26_drive_0018_sync.zip\r\n",
        "! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0019/2011_09_26_drive_0019_sync.zip\r\n",
        "# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0020/2011_09_26_drive_0020_sync.zip\r\n",
        "! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0022/2011_09_26_drive_0022_sync.zip\r\n",
        "! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0023/2011_09_26_drive_0023_sync.zip\r\n",
        "# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0027/2011_09_26_drive_0027_sync.zip\r\n",
        "! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0028/2011_09_26_drive_0028_sync.zip\r\n",
        "# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0029/2011_09_26_drive_0029_sync.zip\r\n",
        "! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0032/2011_09_26_drive_0032_sync.zip\r\n",
        "! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0035/2011_09_26_drive_0035_sync.zip\r\n",
        "! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0036/2011_09_26_drive_0036_sync.zip\r\n",
        "# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0039/2011_09_26_drive_0039_sync.zip\r\n",
        "! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0046/2011_09_26_drive_0046_sync.zip\r\n",
        "! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0048/2011_09_26_drive_0048_sync.zip\r\n",
        "! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0051/2011_09_26_drive_0051_sync.zip\r\n",
        "# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0052/2011_09_26_drive_0052_sync.zip\r\n",
        "# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0056/2011_09_26_drive_0056_sync.zip\r\n",
        "# # ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0057/2011_09_26_drive_0057_sync.zip\r\n",
        "# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0059/2011_09_26_drive_0059_sync.zip\r\n",
        "# # ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0060/2011_09_26_drive_0060_sync.zip\r\n",
        "# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0061/2011_09_26_drive_0061_sync.zip\r\n",
        "# # ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0064/2011_09_26_drive_0064_sync.zip\r\n",
        "# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0070/2011_09_26_drive_0070_sync.zip\r\n",
        "# # ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0079/2011_09_26_drive_0079_sync.zip\r\n",
        "# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0084/2011_09_26_drive_0084_sync.zip\r\n",
        "# # ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0086/2011_09_26_drive_0086_sync.zip\r\n",
        "# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0087/2011_09_26_drive_0087_sync.zip\r\n",
        "# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0091/2011_09_26_drive_0091_sync.zip\r\n",
        "# # ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0093/2011_09_26_drive_0093_sync.zip\r\n",
        "# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0095/2011_09_26_drive_0095_sync.zip\r\n",
        "# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0096/2011_09_26_drive_0096_sync.zip\r\n",
        "# # ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0101/2011_09_26_drive_0101_sync.zip\r\n",
        "# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0104/2011_09_26_drive_0104_sync.zip\r\n",
        "# # ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0106/2011_09_26_drive_0106_sync.zip\r\n",
        "# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0113/2011_09_26_drive_0113_sync.zip\r\n",
        "# # ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0117/2011_09_26_drive_0117_sync.zip\r\n",
        "\r\n",
        "! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_calib.zip\r\n",
        "\"\"\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0001/2011_09_26_drive_0001_sync.zip\\n! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0002/2011_09_26_drive_0002_sync.zip\\n! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0005/2011_09_26_drive_0005_sync.zip\\n! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0009/2011_09_26_drive_0009_sync.zip\\n! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0011/2011_09_26_drive_0011_sync.zip\\n! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0013/2011_09_26_drive_0013_sync.zip\\n# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0014/2011_09_26_drive_0014_sync.zip\\n! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0015/2011_09_26_drive_0015_sync.zip\\n! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0017/2011_09_26_drive_0017_sync.zip\\n# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0018/2011_09_26_drive_0018_sync.zip\\n! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0019/2011_09_26_drive_0019_sync.zip\\n# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0020/2011_09_26_drive_0020_sync.zip\\n! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0022/2011_09_26_drive_0022_sync.zip\\n! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0023/2011_09_26_drive_0023_sync.zip\\n# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0027/2011_09_26_drive_0027_sync.zip\\n! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0028/2011_09_26_drive_0028_sync.zip\\n# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0029/2011_09_26_drive_0029_sync.zip\\n! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0032/2011_09_26_drive_0032_sync.zip\\n! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0035/2011_09_26_drive_0035_sync.zip\\n! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0036/2011_09_26_drive_0036_sync.zip\\n# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0039/2011_09_26_drive_0039_sync.zip\\n! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0046/2011_09_26_drive_0046_sync.zip\\n! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0048/2011_09_26_drive_0048_sync.zip\\n! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0051/2011_09_26_drive_0051_sync.zip\\n# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0052/2011_09_26_drive_0052_sync.zip\\n# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0056/2011_09_26_drive_0056_sync.zip\\n# # ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0057/2011_09_26_drive_0057_sync.zip\\n# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0059/2011_09_26_drive_0059_sync.zip\\n# # ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0060/2011_09_26_drive_0060_sync.zip\\n# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0061/2011_09_26_drive_0061_sync.zip\\n# # ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0064/2011_09_26_drive_0064_sync.zip\\n# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0070/2011_09_26_drive_0070_sync.zip\\n# # ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0079/2011_09_26_drive_0079_sync.zip\\n# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0084/2011_09_26_drive_0084_sync.zip\\n# # ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0086/2011_09_26_drive_0086_sync.zip\\n# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0087/2011_09_26_drive_0087_sync.zip\\n# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0091/2011_09_26_drive_0091_sync.zip\\n# # ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0093/2011_09_26_drive_0093_sync.zip\\n# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0095/2011_09_26_drive_0095_sync.zip\\n# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0096/2011_09_26_drive_0096_sync.zip\\n# # ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0101/2011_09_26_drive_0101_sync.zip\\n# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0104/2011_09_26_drive_0104_sync.zip\\n# # ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0106/2011_09_26_drive_0106_sync.zip\\n# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0113/2011_09_26_drive_0113_sync.zip\\n# # ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0117/2011_09_26_drive_0117_sync.zip\\n\\n! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_calib.zip\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsVkfx9I77aM",
        "outputId": "ea234cba-bf41-4fe7-f250-8f6a36c2aaf1"
      },
      "source": [
        "import torch\r\n",
        "! uname -a\r\n",
        "print(torch.cuda.is_available())\r\n",
        "print(torch.cuda.get_device_name())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linux 8f8e2609b2c4 4.19.112+ #1 SMP Thu Jul 23 08:00:38 PDT 2020 x86_64 x86_64 x86_64 GNU/Linux\n",
            "True\n",
            "Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRh0z4t5QoiJ"
      },
      "source": [
        "# ! python monodepth2/train.py --model_name stereo_model  --frame_ids 0 --use_stereo --split eigen_full"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LaW3qupTEJS"
      },
      "source": [
        "# ! find kitti_data/** -name '*.png' | parallel 'convert -quality 92 -sampling-factor 2x2,1x1,1x1 {.}.png {.}.jpg && rm {}'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuIvPhbqecxe"
      },
      "source": [
        "# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_28_calib.zip\r\n",
        "# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_28_drive_0001/2011_09_28_drive_0001_sync.zip\r\n",
        "# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_28_drive_0002/2011_09_28_drive_0002_sync.zip\r\n",
        "\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXHNpu6ySrUT"
      },
      "source": [
        "# ! rm -rf kitti_data\r\n",
        "# ! mkdir kitti_data\r\n",
        "# ! unzip -q 2011_09_28_drive_0001_sync.zip -d kitti_data\r\n",
        "# ! rm -rf 2011_09_28_drive_0001_sync.zip\r\n",
        "# ! mv data_temp/2011_09_26/2011_09_26_drive_0095_sync/* kitti_data"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5JE0YgXjVe-"
      },
      "source": [
        "# ! sudo apt update\r\n",
        "# ! sudo apt install imagemagick --fix-missing\r\n",
        "# ! convert -h\r\n",
        "# ! find kitti_data/ -name '*.png'\r\n",
        "# ! sudo apt install parallel\r\n",
        "# convert -quality 92 -sampling-factor 2x2,1x1,1x1 kitti_data/2011_09_26/2011_09_26_drive_0048_sync/image_02/data/0000000005.png jpg && rm {}\r\n",
        "# ! find kitti_data/2011_09_28 -name '*.png' | parallel 'convert -quality 92 -sampling-factor 2x2,1x1,1x1 {.}.png {.}.jpg && rm {}'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5yRZNzAj_gx",
        "outputId": "2a5b85e8-7886-4c39-e12e-3b54c23f32e7"
      },
      "source": [
        "# ! python monodepth2/train.py --model_name S_640x192 --frame_ids 0 --use_stereo --pose_model_type separate_resnet --split eigen_full --data_path /content/kitti_data --num_epochs 10\r\n",
        "# /content/kitti_data/2011_09_26/2011_09_26_drive_0106_sync/image_02/data/0000000115.png"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLRxLc8i0-ss"
      },
      "source": [
        "# ! zip -q -r /content/model.zip /root/tmp/S_640x192/*\r\n",
        "# ! ls -la /root/tmp/S_640x192/models/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2l-qT82snuo"
      },
      "source": [
        "# ! python monodepth2/export_gt_depth.py --data_path kitti_data --split eigen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWvl5VeiuppM"
      },
      "source": [
        "# ! python monodepth2/evaluate_depth.py --data_path kitti_data --load_weights_folder /root/tmp/S_640x192/models/weights_19/ --eval_stereo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "prwQU_TPyWLf"
      },
      "source": [
        "\n",
        "# #@title NETWORKS\n",
        "# import torch.nn as nn\n",
        "# import torchvision.models as models\n",
        "# from collections import OrderedDict\n",
        "# import torch.utils.model_zoo as model_zoo\n",
        "# import skimage.transform\n",
        "\n",
        "# class ResNetMultiImageInput(models.ResNet):\n",
        "#     \"\"\"Constructs a resnet model with varying number of input images.\n",
        "#     Adapted from https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
        "#     \"\"\"\n",
        "#     def __init__(self, block, layers, num_classes=1000, num_input_images=1):\n",
        "#         super(ResNetMultiImageInput, self).__init__(block, layers)\n",
        "#         self.inplanes = 64\n",
        "#         self.conv1 = nn.Conv2d(\n",
        "#             num_input_images * 3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "#         self.bn1 = nn.BatchNorm2d(64)\n",
        "#         self.relu = nn.ReLU(inplace=True)\n",
        "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "#         self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "#         self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "#         self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "#         self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "#         for m in self.modules():\n",
        "#             if isinstance(m, nn.Conv2d):\n",
        "#                 nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "#             elif isinstance(m, nn.BatchNorm2d):\n",
        "#                 nn.init.constant_(m.weight, 1)\n",
        "#                 nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "# def resnet_multiimage_input(num_layers, pretrained=False, num_input_images=1):\n",
        "#     \"\"\"Constructs a ResNet model.\n",
        "#     Args:\n",
        "#         num_layers (int): Number of resnet layers. Must be 18 or 50\n",
        "#         pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "#         num_input_images (int): Number of frames stacked as input\n",
        "#     \"\"\"\n",
        "#     assert num_layers in [18, 50], \"Can only run with 18 or 50 layer resnet\"\n",
        "#     blocks = {18: [2, 2, 2, 2], 50: [3, 4, 6, 3]}[num_layers]\n",
        "#     block_type = {18: models.resnet.BasicBlock, 50: models.resnet.Bottleneck}[num_layers]\n",
        "#     model = ResNetMultiImageInput(block_type, blocks, num_input_images=num_input_images)\n",
        "\n",
        "#     if pretrained:\n",
        "#         loaded = model_zoo.load_url(models.resnet.model_urls['resnet{}'.format(num_layers)])\n",
        "#         loaded['conv1.weight'] = torch.cat(\n",
        "#             [loaded['conv1.weight']] * num_input_images, 1) / num_input_images\n",
        "#         model.load_state_dict(loaded)\n",
        "#     return model\n",
        "\n",
        "# class Conv3x3(nn.Module):\n",
        "#     \"\"\"Layer to pad and convolve input\n",
        "#     \"\"\"\n",
        "#     def __init__(self, in_channels, out_channels, use_refl=True):\n",
        "#         super(Conv3x3, self).__init__()\n",
        "\n",
        "#         if use_refl:\n",
        "#             self.pad = nn.ReflectionPad2d(1)\n",
        "#         else:\n",
        "#             self.pad = nn.ZeroPad2d(1)\n",
        "#         self.conv = nn.Conv2d(int(in_channels), int(out_channels), 3)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         out = self.pad(x)\n",
        "#         out = self.conv(out)\n",
        "#         return out\n",
        "\n",
        "\n",
        "# class ConvBlock(nn.Module):\n",
        "#     \"\"\"Layer to perform a convolution followed by ELU\n",
        "#     \"\"\"\n",
        "#     def __init__(self, in_channels, out_channels):\n",
        "#         super(ConvBlock, self).__init__()\n",
        "\n",
        "#         self.conv = Conv3x3(in_channels, out_channels)\n",
        "#         self.nonlin = nn.ELU(inplace=True)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         out = self.conv(x)\n",
        "#         out = self.nonlin(out)\n",
        "#         return out\n",
        "\n",
        "# class ResnetEncoder(nn.Module):\n",
        "#     \"\"\"Pytorch module for a resnet encoder\n",
        "#     \"\"\"\n",
        "#     def __init__(self, num_layers, pretrained, num_input_images=1):\n",
        "#         super(ResnetEncoder, self).__init__()\n",
        "\n",
        "#         self.num_ch_enc = np.array([64, 64, 128, 256, 512])\n",
        "\n",
        "#         resnets = {18: models.resnet18,\n",
        "#                    34: models.resnet34,\n",
        "#                    50: models.resnet50,\n",
        "#                    101: models.resnet101,\n",
        "#                    152: models.resnet152}\n",
        "\n",
        "#         if num_layers not in resnets:\n",
        "#             raise ValueError(\"{} is not a valid number of resnet layers\".format(num_layers))\n",
        "\n",
        "#         if num_input_images > 1:\n",
        "#             self.encoder = resnet_multiimage_input(num_layers, pretrained, num_input_images)\n",
        "#         else:\n",
        "#             self.encoder = resnets[num_layers](pretrained)\n",
        "\n",
        "#         if num_layers > 34:\n",
        "#             self.num_ch_enc[1:] *= 4\n",
        "\n",
        "#     def forward(self, input_image):\n",
        "#         self.features = []\n",
        "#         x = (input_image - 0.45) / 0.225\n",
        "#         x = self.encoder.conv1(x)\n",
        "#         x = self.encoder.bn1(x)\n",
        "#         self.features.append(self.encoder.relu(x))\n",
        "#         self.features.append(self.encoder.layer1(self.encoder.maxpool(self.features[-1])))\n",
        "#         self.features.append(self.encoder.layer2(self.features[-1]))\n",
        "#         self.features.append(self.encoder.layer3(self.features[-1]))\n",
        "#         self.features.append(self.encoder.layer4(self.features[-1]))\n",
        "\n",
        "#         return self.features\n",
        "\n",
        "# class DepthDecoder(nn.Module):\n",
        "#     def __init__(self, num_ch_enc, scales=range(4), num_output_channels=1, use_skips=True):\n",
        "#         super(DepthDecoder, self).__init__()\n",
        "\n",
        "#         self.num_output_channels = num_output_channels\n",
        "#         self.use_skips = use_skips\n",
        "#         self.upsample_mode = 'nearest'\n",
        "#         self.scales = scales\n",
        "\n",
        "#         self.num_ch_enc = num_ch_enc\n",
        "#         self.num_ch_dec = np.array([16, 32, 64, 128, 256])\n",
        "\n",
        "#         # decoder\n",
        "#         self.convs = OrderedDict()\n",
        "#         for i in range(4, -1, -1):\n",
        "#             # upconv_0\n",
        "#             num_ch_in = self.num_ch_enc[-1] if i == 4 else self.num_ch_dec[i + 1]\n",
        "#             num_ch_out = self.num_ch_dec[i]\n",
        "#             self.convs[(\"upconv\", i, 0)] = ConvBlock(num_ch_in, num_ch_out)\n",
        "\n",
        "#             # upconv_1\n",
        "#             num_ch_in = self.num_ch_dec[i]\n",
        "#             if self.use_skips and i > 0:\n",
        "#                 num_ch_in += self.num_ch_enc[i - 1]\n",
        "#             num_ch_out = self.num_ch_dec[i]\n",
        "#             self.convs[(\"upconv\", i, 1)] = ConvBlock(num_ch_in, num_ch_out)\n",
        "\n",
        "#         for s in self.scales:\n",
        "#             self.convs[(\"dispconv\", s)] = Conv3x3(self.num_ch_dec[s], self.num_output_channels)\n",
        "\n",
        "#         self.decoder = nn.ModuleList(list(self.convs.values()))\n",
        "#         self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "#     def forward(self, input_features):\n",
        "#         self.outputs = {}\n",
        "\n",
        "#         # decoder\n",
        "#         x = input_features[-1]\n",
        "#         for i in range(4, -1, -1):\n",
        "#             x = self.convs[(\"upconv\", i, 0)](x)\n",
        "#             x = [upsample(x)]\n",
        "#             if self.use_skips and i > 0:\n",
        "#                 x += [input_features[i - 1]]\n",
        "#             x = torch.cat(x, 1)\n",
        "#             x = self.convs[(\"upconv\", i, 1)](x)\n",
        "#             if i in self.scales:\n",
        "#                 self.outputs[(\"disp\", i)] = self.sigmoid(self.convs[(\"dispconv\", i)](x))\n",
        "\n",
        "#         return self.outputs\n",
        "\n",
        "# class PoseDecoder(nn.Module):\n",
        "#     def __init__(self, num_ch_enc, num_input_features, num_frames_to_predict_for=None, stride=1):\n",
        "#         super(PoseDecoder, self).__init__()\n",
        "\n",
        "#         self.num_ch_enc = num_ch_enc\n",
        "#         self.num_input_features = num_input_features\n",
        "\n",
        "#         if num_frames_to_predict_for is None:\n",
        "#             num_frames_to_predict_for = num_input_features - 1\n",
        "#         self.num_frames_to_predict_for = num_frames_to_predict_for\n",
        "\n",
        "#         self.convs = OrderedDict()\n",
        "#         self.convs[(\"squeeze\")] = nn.Conv2d(self.num_ch_enc[-1], 256, 1)\n",
        "#         self.convs[(\"pose\", 0)] = nn.Conv2d(num_input_features * 256, 256, 3, stride, 1)\n",
        "#         self.convs[(\"pose\", 1)] = nn.Conv2d(256, 256, 3, stride, 1)\n",
        "#         self.convs[(\"pose\", 2)] = nn.Conv2d(256, 6 * num_frames_to_predict_for, 1)\n",
        "\n",
        "#         self.relu = nn.ReLU()\n",
        "\n",
        "#         self.net = nn.ModuleList(list(self.convs.values()))\n",
        "\n",
        "#     def forward(self, input_features):\n",
        "#         last_features = [f[-1] for f in input_features]\n",
        "\n",
        "#         cat_features = [self.relu(self.convs[\"squeeze\"](f)) for f in last_features]\n",
        "#         cat_features = torch.cat(cat_features, 1)\n",
        "\n",
        "#         out = cat_features\n",
        "#         for i in range(3):\n",
        "#             out = self.convs[(\"pose\", i)](out)\n",
        "#             if i != 2:\n",
        "#                 out = self.relu(out)\n",
        "\n",
        "#         out = out.mean(3).mean(2)\n",
        "\n",
        "#         out = 0.01 * out.view(-1, self.num_frames_to_predict_for, 1, 6)\n",
        "\n",
        "#         axisangle = out[..., :3]\n",
        "#         translation = out[..., 3:]\n",
        "\n",
        "#         return axisangle, translation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOnW16nVP0Fv"
      },
      "source": [
        "# import os\r\n",
        "# import random\r\n",
        "# import numpy as np\r\n",
        "# import copy\r\n",
        "# from PIL import Image  # using pillow-simd for increased speed\r\n",
        "\r\n",
        "# import torch\r\n",
        "# import torch.utils.data as data\r\n",
        "# from torchvision import transforms\r\n",
        "# from collections import Counter\r\n",
        "\r\n",
        "# class BackprojectDepth(nn.Module):\r\n",
        "#     \"\"\"Layer to transform a depth image into a point cloud\r\n",
        "#     \"\"\"\r\n",
        "#     def __init__(self, batch_size, height, width):\r\n",
        "#         super(BackprojectDepth, self).__init__()\r\n",
        "\r\n",
        "#         self.batch_size = batch_size\r\n",
        "#         self.height = height\r\n",
        "#         self.width = width\r\n",
        "\r\n",
        "#         meshgrid = np.meshgrid(range(self.width), range(self.height), indexing='xy')\r\n",
        "#         self.id_coords = np.stack(meshgrid, axis=0).astype(np.float32)\r\n",
        "#         self.id_coords = nn.Parameter(torch.from_numpy(self.id_coords),\r\n",
        "#                                       requires_grad=False)\r\n",
        "\r\n",
        "#         self.ones = nn.Parameter(torch.ones(self.batch_size, 1, self.height * self.width),\r\n",
        "#                                  requires_grad=False)\r\n",
        "\r\n",
        "#         self.pix_coords = torch.unsqueeze(torch.stack(\r\n",
        "#             [self.id_coords[0].view(-1), self.id_coords[1].view(-1)], 0), 0)\r\n",
        "#         self.pix_coords = self.pix_coords.repeat(batch_size, 1, 1)\r\n",
        "#         self.pix_coords = nn.Parameter(torch.cat([self.pix_coords, self.ones], 1),\r\n",
        "#                                        requires_grad=False)\r\n",
        "\r\n",
        "#     def forward(self, depth, inv_K):\r\n",
        "#         cam_points = torch.matmul(inv_K[:, :3, :3], self.pix_coords)\r\n",
        "#         cam_points = depth.view(self.batch_size, 1, -1) * cam_points\r\n",
        "#         cam_points = torch.cat([cam_points, self.ones], 1)\r\n",
        "\r\n",
        "#         return cam_points\r\n",
        "\r\n",
        "\r\n",
        "# class Project3D(nn.Module):\r\n",
        "#     \"\"\"Layer which projects 3D points into a camera with intrinsics K and at position T\r\n",
        "#     \"\"\"\r\n",
        "#     def __init__(self, batch_size, height, width, eps=1e-7):\r\n",
        "#         super(Project3D, self).__init__()\r\n",
        "\r\n",
        "#         self.batch_size = batch_size\r\n",
        "#         self.height = height\r\n",
        "#         self.width = width\r\n",
        "#         self.eps = eps\r\n",
        "\r\n",
        "#     def forward(self, points, K, T):\r\n",
        "#         P = torch.matmul(K, T)[:, :3, :]\r\n",
        "\r\n",
        "#         cam_points = torch.matmul(P, points)\r\n",
        "\r\n",
        "#         pix_coords = cam_points[:, :2, :] / (cam_points[:, 2, :].unsqueeze(1) + self.eps)\r\n",
        "#         pix_coords = pix_coords.view(self.batch_size, 2, self.height, self.width)\r\n",
        "#         pix_coords = pix_coords.permute(0, 2, 3, 1)\r\n",
        "#         pix_coords[..., 0] /= self.width - 1\r\n",
        "#         pix_coords[..., 1] /= self.height - 1\r\n",
        "#         pix_coords = (pix_coords - 0.5) * 2\r\n",
        "#         return pix_coords\r\n",
        "\r\n",
        "# def pil_loader(path):\r\n",
        "#     # open path as file to avoid ResourceWarning\r\n",
        "#     # (https://github.com/python-pillow/Pillow/issues/835)\r\n",
        "#     with open(path, 'rb') as f:\r\n",
        "#         with Image.open(f) as img:\r\n",
        "#             return img.convert('RGB')\r\n",
        "\r\n",
        "\r\n",
        "# class MonoDataset(data.Dataset):\r\n",
        "#     \"\"\"Superclass for monocular dataloaders\r\n",
        "\r\n",
        "#     Args:\r\n",
        "#         data_path\r\n",
        "#         filenames\r\n",
        "#         height\r\n",
        "#         width\r\n",
        "#         frame_idxs\r\n",
        "#         num_scales\r\n",
        "#         is_train\r\n",
        "#         img_ext\r\n",
        "#     \"\"\"\r\n",
        "#     def __init__(self,\r\n",
        "#                  data_path,\r\n",
        "#                  filenames,\r\n",
        "#                  height,\r\n",
        "#                  width,\r\n",
        "#                  frame_idxs,\r\n",
        "#                  num_scales,\r\n",
        "#                  is_train=False,\r\n",
        "#                  img_ext='.jpg'):\r\n",
        "#         super(MonoDataset, self).__init__()\r\n",
        "\r\n",
        "#         self.data_path = data_path\r\n",
        "#         self.filenames = filenames\r\n",
        "#         self.height = height\r\n",
        "#         self.width = width\r\n",
        "#         self.num_scales = num_scales\r\n",
        "#         self.interp = Image.ANTIALIAS\r\n",
        "\r\n",
        "#         self.frame_idxs = frame_idxs\r\n",
        "\r\n",
        "#         self.is_train = is_train\r\n",
        "#         self.img_ext = img_ext\r\n",
        "\r\n",
        "#         self.loader = pil_loader\r\n",
        "#         self.to_tensor = transforms.ToTensor()\r\n",
        "\r\n",
        "#         # We need to specify augmentations differently in newer versions of torchvision.\r\n",
        "#         # We first try the newer tuple version; if this fails we fall back to scalars\r\n",
        "#         try:\r\n",
        "#             self.brightness = (0.8, 1.2)\r\n",
        "#             self.contrast = (0.8, 1.2)\r\n",
        "#             self.saturation = (0.8, 1.2)\r\n",
        "#             self.hue = (-0.1, 0.1)\r\n",
        "#             transforms.ColorJitter.get_params(\r\n",
        "#                 self.brightness, self.contrast, self.saturation, self.hue)\r\n",
        "#         except TypeError:\r\n",
        "#             self.brightness = 0.2\r\n",
        "#             self.contrast = 0.2\r\n",
        "#             self.saturation = 0.2\r\n",
        "#             self.hue = 0.1\r\n",
        "\r\n",
        "#         self.resize = {}\r\n",
        "#         for i in range(self.num_scales):\r\n",
        "#             s = 2 ** i\r\n",
        "#             self.resize[i] = transforms.Resize((self.height // s, self.width // s),\r\n",
        "#                                                interpolation=self.interp)\r\n",
        "\r\n",
        "#         self.load_depth = self.check_depth()\r\n",
        "\r\n",
        "#     def preprocess(self, inputs, color_aug):\r\n",
        "#         \"\"\"Resize colour images to the required scales and augment if required\r\n",
        "\r\n",
        "#         We create the color_aug object in advance and apply the same augmentation to all\r\n",
        "#         images in this item. This ensures that all images input to the pose network receive the\r\n",
        "#         same augmentation.\r\n",
        "#         \"\"\"\r\n",
        "#         for k in list(inputs):\r\n",
        "#             frame = inputs[k]\r\n",
        "#             if \"color\" in k:\r\n",
        "#                 n, im, i = k\r\n",
        "#                 for i in range(self.num_scales):\r\n",
        "#                     inputs[(n, im, i)] = self.resize[i](inputs[(n, im, i - 1)])\r\n",
        "\r\n",
        "#         for k in list(inputs):\r\n",
        "#             f = inputs[k]\r\n",
        "#             if \"color\" in k:\r\n",
        "#                 n, im, i = k\r\n",
        "#                 inputs[(n, im, i)] = self.to_tensor(f)\r\n",
        "#                 inputs[(n + \"_aug\", im, i)] = self.to_tensor(color_aug(f))\r\n",
        "\r\n",
        "#     def __len__(self):\r\n",
        "#         return len(self.filenames)\r\n",
        "\r\n",
        "#     def __getitem__(self, index):\r\n",
        "#         \"\"\"Returns a single training item from the dataset as a dictionary.\r\n",
        "\r\n",
        "#         Values correspond to torch tensors.\r\n",
        "#         Keys in the dictionary are either strings or tuples:\r\n",
        "\r\n",
        "#             (\"color\", <frame_id>, <scale>)          for raw colour images,\r\n",
        "#             (\"color_aug\", <frame_id>, <scale>)      for augmented colour images,\r\n",
        "#             (\"K\", scale) or (\"inv_K\", scale)        for camera intrinsics,\r\n",
        "#             \"stereo_T\"                              for camera extrinsics, and\r\n",
        "#             \"depth_gt\"                              for ground truth depth maps.\r\n",
        "\r\n",
        "#         <frame_id> is either:\r\n",
        "#             an integer (e.g. 0, -1, or 1) representing the temporal step relative to 'index',\r\n",
        "#         or\r\n",
        "#             \"s\" for the opposite image in the stereo pair.\r\n",
        "\r\n",
        "#         <scale> is an integer representing the scale of the image relative to the fullsize image:\r\n",
        "#             -1      images at native resolution as loaded from disk\r\n",
        "#             0       images resized to (self.width,      self.height     )\r\n",
        "#             1       images resized to (self.width // 2, self.height // 2)\r\n",
        "#             2       images resized to (self.width // 4, self.height // 4)\r\n",
        "#             3       images resized to (self.width // 8, self.height // 8)\r\n",
        "#         \"\"\"\r\n",
        "#         inputs = {}\r\n",
        "\r\n",
        "#         do_color_aug = self.is_train and random.random() > 0.5\r\n",
        "#         do_flip = self.is_train and random.random() > 0.5\r\n",
        "\r\n",
        "#         line = self.filenames[index].split()\r\n",
        "#         folder = line[0]\r\n",
        "\r\n",
        "#         if len(line) == 3:\r\n",
        "#             frame_index = int(line[1])\r\n",
        "#         else:\r\n",
        "#             frame_index = 0\r\n",
        "\r\n",
        "#         if len(line) == 3:\r\n",
        "#             side = line[2]\r\n",
        "#         else:\r\n",
        "#             side = None\r\n",
        "\r\n",
        "#         for i in self.frame_idxs:\r\n",
        "#             if i == \"s\":\r\n",
        "#                 other_side = {\"r\": \"l\", \"l\": \"r\"}[side]\r\n",
        "#                 inputs[(\"color\", i, -1)] = self.get_color(folder, frame_index, other_side, do_flip)\r\n",
        "#             else:\r\n",
        "#                 inputs[(\"color\", i, -1)] = self.get_color(folder, frame_index + i, side, do_flip)\r\n",
        "\r\n",
        "#         # adjusting intrinsics to match each scale in the pyramid\r\n",
        "#         for scale in range(self.num_scales):\r\n",
        "#             K = self.K.copy()\r\n",
        "\r\n",
        "#             K[0, :] *= self.width // (2 ** scale)\r\n",
        "#             K[1, :] *= self.height // (2 ** scale)\r\n",
        "\r\n",
        "#             inv_K = np.linalg.pinv(K)\r\n",
        "\r\n",
        "#             inputs[(\"K\", scale)] = torch.from_numpy(K)\r\n",
        "#             inputs[(\"inv_K\", scale)] = torch.from_numpy(inv_K)\r\n",
        "\r\n",
        "#         if do_color_aug:\r\n",
        "#             color_aug = transforms.ColorJitter.get_params(\r\n",
        "#                 self.brightness, self.contrast, self.saturation, self.hue)\r\n",
        "#         else:\r\n",
        "#             color_aug = (lambda x: x)\r\n",
        "\r\n",
        "#         self.preprocess(inputs, color_aug)\r\n",
        "\r\n",
        "#         for i in self.frame_idxs:\r\n",
        "#             del inputs[(\"color\", i, -1)]\r\n",
        "#             del inputs[(\"color_aug\", i, -1)]\r\n",
        "\r\n",
        "#         if self.load_depth:\r\n",
        "#             depth_gt = self.get_depth(folder, frame_index, side, do_flip)\r\n",
        "#             inputs[\"depth_gt\"] = np.expand_dims(depth_gt, 0)\r\n",
        "#             inputs[\"depth_gt\"] = torch.from_numpy(inputs[\"depth_gt\"].astype(np.float32))\r\n",
        "\r\n",
        "#         if \"s\" in self.frame_idxs:\r\n",
        "#             stereo_T = np.eye(4, dtype=np.float32)\r\n",
        "#             baseline_sign = -1 if do_flip else 1\r\n",
        "#             side_sign = -1 if side == \"l\" else 1\r\n",
        "#             stereo_T[0, 3] = side_sign * baseline_sign * 0.1\r\n",
        "\r\n",
        "#             inputs[\"stereo_T\"] = torch.from_numpy(stereo_T)\r\n",
        "\r\n",
        "#         return inputs\r\n",
        "\r\n",
        "#     def get_color(self, folder, frame_index, side, do_flip):\r\n",
        "#         raise NotImplementedError\r\n",
        "\r\n",
        "#     def check_depth(self):\r\n",
        "#         raise NotImplementedError\r\n",
        "\r\n",
        "#     def get_depth(self, folder, frame_index, side, do_flip):\r\n",
        "#         raise NotImplementedError\r\n",
        "\r\n",
        "# class KITTIDataset(MonoDataset):\r\n",
        "#     \"\"\"Superclass for different types of KITTI dataset loaders\r\n",
        "#     \"\"\"\r\n",
        "#     def __init__(self, *args, **kwargs):\r\n",
        "#         super(KITTIDataset, self).__init__(*args, **kwargs)\r\n",
        "\r\n",
        "#         # NOTE: Make sure your intrinsics matrix is *normalized* by the original image size    \r\n",
        "#         self.K = np.array([[0.58, 0, 0.5, 0],\r\n",
        "#                            [0, 1.92, 0.5, 0],\r\n",
        "#                            [0, 0, 1, 0],\r\n",
        "#                            [0, 0, 0, 1]], dtype=np.float32)\r\n",
        "\r\n",
        "#         self.full_res_shape = (1242, 375)\r\n",
        "#         self.side_map = {\"2\": 2, \"3\": 3, \"l\": 2, \"r\": 3}\r\n",
        "\r\n",
        "#     def check_depth(self):\r\n",
        "#         line = self.filenames[0].split()\r\n",
        "#         scene_name = line[0]\r\n",
        "#         frame_index = int(line[1])\r\n",
        "\r\n",
        "#         velo_filename = os.path.join(\r\n",
        "#             self.data_path,\r\n",
        "#             scene_name,\r\n",
        "#             \"velodyne_points/data/{:010d}.bin\".format(int(frame_index)))\r\n",
        "\r\n",
        "#         return os.path.isfile(velo_filename)\r\n",
        "\r\n",
        "#     def get_color(self, folder, frame_index, side, do_flip):\r\n",
        "#         color = self.loader(self.get_image_path(folder, frame_index, side))\r\n",
        "\r\n",
        "#         if do_flip:\r\n",
        "#             color = color.transpose(pil.FLIP_LEFT_RIGHT)\r\n",
        "\r\n",
        "#         return color\r\n",
        "\r\n",
        "\r\n",
        "# class KITTIRAWDataset(KITTIDataset):\r\n",
        "#     \"\"\"KITTI dataset which loads the original velodyne depth maps for ground truth\r\n",
        "#     \"\"\"\r\n",
        "#     def __init__(self, *args, **kwargs):\r\n",
        "#         super(KITTIRAWDataset, self).__init__(*args, **kwargs)\r\n",
        "\r\n",
        "#     def get_image_path(self, folder, frame_index, side):\r\n",
        "#         f_str = \"{:010d}{}\".format(frame_index, self.img_ext)\r\n",
        "#         image_path = os.path.join(\r\n",
        "#             self.data_path, folder, \"image_0{}/data\".format(self.side_map[side]), f_str)\r\n",
        "#         return image_path\r\n",
        "\r\n",
        "#     def get_depth(self, folder, frame_index, side, do_flip):\r\n",
        "#         calib_path = os.path.join(self.data_path, folder.split(\"/\")[0])\r\n",
        "\r\n",
        "#         velo_filename = os.path.join(\r\n",
        "#             self.data_path,\r\n",
        "#             folder,\r\n",
        "#             \"velodyne_points/data/{:010d}.bin\".format(int(frame_index)))\r\n",
        "\r\n",
        "#         depth_gt = generate_depth_map(calib_path, velo_filename, self.side_map[side])\r\n",
        "#         depth_gt = skimage.transform.resize(\r\n",
        "#             depth_gt, self.full_res_shape[::-1], order=0, preserve_range=True, mode='constant')\r\n",
        "\r\n",
        "#         if do_flip:\r\n",
        "#             depth_gt = np.fliplr(depth_gt)\r\n",
        "\r\n",
        "#         return depth_gt\r\n",
        "\r\n",
        "# class SSIM(nn.Module):\r\n",
        "#     \"\"\"Layer to compute the SSIM loss between a pair of images\r\n",
        "#     \"\"\"\r\n",
        "#     def __init__(self):\r\n",
        "#         super(SSIM, self).__init__()\r\n",
        "#         self.mu_x_pool   = nn.AvgPool2d(3, 1)\r\n",
        "#         self.mu_y_pool   = nn.AvgPool2d(3, 1)\r\n",
        "#         self.sig_x_pool  = nn.AvgPool2d(3, 1)\r\n",
        "#         self.sig_y_pool  = nn.AvgPool2d(3, 1)\r\n",
        "#         self.sig_xy_pool = nn.AvgPool2d(3, 1)\r\n",
        "\r\n",
        "#         self.refl = nn.ReflectionPad2d(1)\r\n",
        "\r\n",
        "#         self.C1 = 0.01 ** 2\r\n",
        "#         self.C2 = 0.03 ** 2\r\n",
        "\r\n",
        "#     def forward(self, x, y):\r\n",
        "#         x = self.refl(x)\r\n",
        "#         y = self.refl(y)\r\n",
        "\r\n",
        "#         mu_x = self.mu_x_pool(x)\r\n",
        "#         mu_y = self.mu_y_pool(y)\r\n",
        "\r\n",
        "#         sigma_x  = self.sig_x_pool(x ** 2) - mu_x ** 2\r\n",
        "#         sigma_y  = self.sig_y_pool(y ** 2) - mu_y ** 2\r\n",
        "#         sigma_xy = self.sig_xy_pool(x * y) - mu_x * mu_y\r\n",
        "\r\n",
        "#         SSIM_n = (2 * mu_x * mu_y + self.C1) * (2 * sigma_xy + self.C2)\r\n",
        "#         SSIM_d = (mu_x ** 2 + mu_y ** 2 + self.C1) * (sigma_x + sigma_y + self.C2)\r\n",
        "\r\n",
        "#         return torch.clamp((1 - SSIM_n / SSIM_d) / 2, 0, 1)\r\n",
        "\r\n",
        "# def generate_depth_map(calib_dir, velo_filename, cam=2, vel_depth=False):\r\n",
        "#     \"\"\"Generate a depth map from velodyne data\r\n",
        "#     \"\"\"\r\n",
        "#     # load calibration files\r\n",
        "#     cam2cam = read_calib_file(os.path.join(calib_dir, 'calib_cam_to_cam.txt'))\r\n",
        "#     velo2cam = read_calib_file(os.path.join(calib_dir, 'calib_velo_to_cam.txt'))\r\n",
        "#     velo2cam = np.hstack((velo2cam['R'].reshape(3, 3), velo2cam['T'][..., np.newaxis]))\r\n",
        "#     velo2cam = np.vstack((velo2cam, np.array([0, 0, 0, 1.0])))\r\n",
        "\r\n",
        "#     # get image shape\r\n",
        "#     im_shape = cam2cam[\"S_rect_02\"][::-1].astype(np.int32)\r\n",
        "\r\n",
        "#     # compute projection matrix velodyne->image plane\r\n",
        "#     R_cam2rect = np.eye(4)\r\n",
        "#     R_cam2rect[:3, :3] = cam2cam['R_rect_00'].reshape(3, 3)\r\n",
        "#     P_rect = cam2cam['P_rect_0'+str(cam)].reshape(3, 4)\r\n",
        "#     P_velo2im = np.dot(np.dot(P_rect, R_cam2rect), velo2cam)\r\n",
        "\r\n",
        "#     # load velodyne points and remove all behind image plane (approximation)\r\n",
        "#     # each row of the velodyne data is forward, left, up, reflectance\r\n",
        "#     velo = load_velodyne_points(velo_filename)\r\n",
        "#     velo = velo[velo[:, 0] >= 0, :]\r\n",
        "\r\n",
        "#     # project the points to the camera\r\n",
        "#     velo_pts_im = np.dot(P_velo2im, velo.T).T\r\n",
        "#     velo_pts_im[:, :2] = velo_pts_im[:, :2] / velo_pts_im[:, 2][..., np.newaxis]\r\n",
        "\r\n",
        "#     if vel_depth:\r\n",
        "#         velo_pts_im[:, 2] = velo[:, 0]\r\n",
        "\r\n",
        "#     # check if in bounds\r\n",
        "#     # use minus 1 to get the exact same value as KITTI matlab code\r\n",
        "#     velo_pts_im[:, 0] = np.round(velo_pts_im[:, 0]) - 1\r\n",
        "#     velo_pts_im[:, 1] = np.round(velo_pts_im[:, 1]) - 1\r\n",
        "#     val_inds = (velo_pts_im[:, 0] >= 0) & (velo_pts_im[:, 1] >= 0)\r\n",
        "#     val_inds = val_inds & (velo_pts_im[:, 0] < im_shape[1]) & (velo_pts_im[:, 1] < im_shape[0])\r\n",
        "#     velo_pts_im = velo_pts_im[val_inds, :]\r\n",
        "\r\n",
        "#     # project to image\r\n",
        "#     depth = np.zeros((im_shape[:2]))\r\n",
        "#     depth[velo_pts_im[:, 1].astype(np.int), velo_pts_im[:, 0].astype(np.int)] = velo_pts_im[:, 2]\r\n",
        "\r\n",
        "#     # find the duplicate points and choose the closest depth\r\n",
        "#     inds = sub2ind(depth.shape, velo_pts_im[:, 1], velo_pts_im[:, 0])\r\n",
        "#     dupe_inds = [item for item, count in Counter(inds).items() if count > 1]\r\n",
        "#     for dd in dupe_inds:\r\n",
        "#         pts = np.where(inds == dd)[0]\r\n",
        "#         x_loc = int(velo_pts_im[pts[0], 0])\r\n",
        "#         y_loc = int(velo_pts_im[pts[0], 1])\r\n",
        "#         depth[y_loc, x_loc] = velo_pts_im[pts, 2].min()\r\n",
        "#     depth[depth < 0] = 0\r\n",
        "\r\n",
        "#     return depth\r\n",
        "\r\n",
        "# def read_calib_file(path):\r\n",
        "#     \"\"\"Read KITTI calibration file\r\n",
        "#     (from https://github.com/hunse/kitti)\r\n",
        "#     \"\"\"\r\n",
        "#     float_chars = set(\"0123456789.e+- \")\r\n",
        "#     data = {}\r\n",
        "#     with open(path, 'r') as f:\r\n",
        "#         for line in f.readlines():\r\n",
        "#             key, value = line.split(':', 1)\r\n",
        "#             value = value.strip()\r\n",
        "#             data[key] = value\r\n",
        "#             if float_chars.issuperset(value):\r\n",
        "#                 # try to cast to float array\r\n",
        "#                 try:\r\n",
        "#                     data[key] = np.array(list(map(float, value.split(' '))))\r\n",
        "#                 except ValueError:\r\n",
        "#                     # casting error: data[key] already eq. value, so pass\r\n",
        "#                     pass\r\n",
        "\r\n",
        "#     return data\r\n",
        "\r\n",
        "# def load_velodyne_points(filename):\r\n",
        "#     \"\"\"Load 3D point cloud from KITTI file format\r\n",
        "#     (adapted from https://github.com/hunse/kitti)\r\n",
        "#     \"\"\"\r\n",
        "#     points = np.fromfile(filename, dtype=np.float32).reshape(-1, 4)\r\n",
        "#     points[:, 3] = 1.0  # homogeneous\r\n",
        "#     return points\r\n",
        "\r\n",
        "# def sub2ind(matrixSize, rowSub, colSub):\r\n",
        "#     \"\"\"Convert row, col matrix subscripts to linear indices\r\n",
        "#     \"\"\"\r\n",
        "#     m, n = matrixSize\r\n",
        "#     return rowSub * (n-1) + colSub - 1\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "c7mdtNn2xC6I"
      },
      "source": [
        "#@title Default title text\n",
        "### For Training\n",
        "import monodepth2.layers, monodepth2.utils, monodepth2.trainer\n",
        "import monodepth2.networks\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import timeit\n",
        "import PIL.Image as pil\n",
        "\n",
        "\n",
        "class MyTraining(object):\n",
        "    def __init__(self):\n",
        "        self.h = 192\n",
        "        self.w = 640\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.device(\"cuda\")\n",
        "        else:\n",
        "            self.device = torch.device(\"cpu\")\n",
        "        self.frame_diffs = [0, 's']\n",
        "\n",
        "        self.loss_scales = [0, 1, 2, 3]\n",
        "\n",
        "        # Define projections for pose estimation\n",
        "        self.depth_projection = {}\n",
        "        self.projection_3d = {}\n",
        "        for loss_scale in self.loss_scales:\n",
        "            h = int(self.h / (2 ** loss_scale))\n",
        "            w = int(self.w / (2 ** loss_scale))\n",
        "\n",
        "            self.depth_projection[loss_scale] = monodepth2.layers.BackprojectDepth(6, h, w).to(self.device)\n",
        "            self.projection_3d[loss_scale] = monodepth2.layers.Project3D(6, h, w).to(self.device)\n",
        "        \n",
        "        \n",
        "        # Declare 2 Neural Networks\n",
        "\n",
        "        ## Declare Depth Network\n",
        "        self.depth_encoder_network = monodepth2.networks.ResnetEncoder(18, True).to(self.device)\n",
        "        self.depth_decoder_network = monodepth2.networks.DepthDecoder(self.depth_encoder_network.num_ch_enc, \n",
        "                                                [0, 1, 2, 3]).to(self.device) \n",
        "\n",
        "        ## Declare Pose Network\n",
        "        self.pose_encoder_network = monodepth2.networks.ResnetEncoder(18, True, num_input_images=2).to(self.device)\n",
        "        self.pose_decoder_network = monodepth2.networks.PoseDecoder(self.pose_encoder_network.num_ch_enc, \n",
        "                                                num_input_features=1, num_frames_to_predict_for=2).to(self.device)\n",
        "\n",
        "        self.models = {\"encoder\": self.depth_encoder_network, \"depth\": self.depth_decoder_network,\n",
        "                       \"pose_encoder\": self.pose_encoder_network, \"pose\": self.pose_decoder_network}\n",
        "\n",
        "        # Predictive Mask Not Used\n",
        "\n",
        "        # Not using load_model feature\n",
        "\n",
        "        # Set up our excerpt of Kitti Dataset\n",
        "        training_files = monodepth2.utils.readlines(\"/content/monodepth2/splits/eigen_full/train_files.txt\")\n",
        "        validation_files = monodepth2.utils.readlines(\"/content/monodepth2/splits/eigen_full/val_files.txt\")\n",
        "\n",
        "        train_set = monodepth2.datasets.kitti_dataset.KITTIRAWDataset(\"/content/kitti_data\", training_files, self.h, self.w, self.frame_diffs, 4, is_train=True, img_ext='.jpg')\n",
        "        self.train_loader = DataLoader(train_set, 6, True, num_workers=1, pin_memory=True, drop_last=True)\n",
        "        val_set = monodepth2.datasets.kitti_dataset.KITTIRAWDataset(\"/content/kitti_data\", validation_files, self.h, self.w, self.frame_diffs, 4, is_train=False, img_ext='.jpg')\n",
        "        self.val_loader = DataLoader(val_set, 6, True, num_workers=1, pin_memory=True, drop_last=True)\n",
        "\n",
        "    def run_training_loop(self):\n",
        "        self.all_params = list(self.depth_encoder_network.parameters())\n",
        "        self.all_params += list(self.depth_decoder_network.parameters())\n",
        "        self.all_params += list(self.pose_encoder_network.parameters())\n",
        "        self.all_params += list(self.pose_decoder_network.parameters())\n",
        "\n",
        "        self.adam_optim = optim.Adam(self.all_params, 1e-4)\n",
        "        self.lr_sched = optim.lr_scheduler.StepLR(self.adam_optim, 15, 0.1)\n",
        "        self.ssim_loss_func = monodepth2.layers.SSIM().to(self.device)\n",
        "\n",
        "        self.epoch_count = 0\n",
        "        self.step_count = 0\n",
        "        self.start = timeit.default_timer()\n",
        "\n",
        "        losses  = []\n",
        "\n",
        "        for self.epoch_count in range(2):\n",
        "            # Per Epoch\n",
        "            self.lr_sched.step()\n",
        "            self.depth_encoder_network.train()\n",
        "            self.depth_decoder_network.train()\n",
        "            self.pose_encoder_network.train()\n",
        "            self.pose_decoder_network.train()\n",
        "\n",
        "            for idx, inputs in enumerate(self.train_loader):\n",
        "                for key, ipt in inputs.items():\n",
        "                    inputs[key] = ipt.to(self.device)\n",
        "\n",
        "                # Per Batch Code\n",
        "                batch_start_time = timeit.default_timer()\n",
        "\n",
        "                feature_identifications = self.depth_encoder_network(inputs[\"color_aug\", 0, 0])\n",
        "                outputs = self.depth_decoder_network(feature_identifications)\n",
        "\n",
        "                self.estimate_stereo_predictions(inputs, outputs)\n",
        "                loss = self.batch_loss_func(inputs, outputs)\n",
        "\n",
        "                self.adam_optim.zero_grad()\n",
        "                loss[\"loss\"].backward()\n",
        "                losses.append(loss[\"loss\"].item())\n",
        "                self.adam_optim.step()\n",
        "\n",
        "                batch_duration = timeit.default_timer() - batch_start_time\n",
        "\n",
        "                # Do something with batch duration and losses\n",
        "\n",
        "                self.step_count += 1\n",
        "\n",
        "    def estimate_stereo_predictions(self, inputs, outputs):\n",
        "        # Generate estimated stereo pair using the pose networks\n",
        "        for loss_scale in self.loss_scales:\n",
        "            estimated_disparity = outputs[(\"disp\", loss_scale)]\n",
        "            disp = F.interpolate(estimated_disparity, [self.h, self.w], mode=\"bilinear\", align_corners=False)\n",
        "            base = 0 # base scale\n",
        "\n",
        "            # Change names\n",
        "            scaled_disp = 0.001 + (10 - 0.001) * disp\n",
        "            depth = 1 / scaled_disp\n",
        "\n",
        "            outputs[(\"depth\", 0, loss_scale)] = depth\n",
        "\n",
        "            # Tweak this code once I understand things a bit more\n",
        "            for idx, id in enumerate(self.frame_diffs[1:]):\n",
        "                if id == \"s\":\n",
        "                    T = inputs[\"stereo_T\"]\n",
        "                else:\n",
        "                    T = outputs[(\"cam_T_cam\", 0, id)]\n",
        "\n",
        "                camera_coords = self.depth_projection[base](depth, inputs[(\"inv_K\", base)])\n",
        "                pixel_coords = self.projection_3d[base](camera_coords, inputs[(\"K\", base)], T)\n",
        "\n",
        "                outputs[(\"sample\", id, loss_scale)] = pixel_coords\n",
        "                outputs[(\"color\", id, loss_scale)] = F.grid_sample(\n",
        "                    inputs[(\"color\", id, base)], outputs[(\"sample\", id, loss_scale)], padding_mode=\"border\")\n",
        "                \n",
        "    def reprojection(self, prediction, target):\n",
        "        ssim_loss = self.ssim_loss_func(prediction, target).mean(1, True)\n",
        "        reproj_losses = 0.85*ssim_loss + 0.15*(torch.abs(target-prediction).mean(1, True))\n",
        "        return reproj_losses\n",
        "\n",
        "    def batch_loss_func(self, inputs, outputs):\n",
        "        batch_loss = {}\n",
        "        complete_losses = 0\n",
        "\n",
        "        for loss_scale in self.loss_scales:\n",
        "            current_loss = 0\n",
        "\n",
        "            base = 0\n",
        "\n",
        "            reproj_losses = []\n",
        "\n",
        "            for id in self.frame_diffs[1:]:\n",
        "                reproj_losses.append(self.reprojection(outputs[(\"color\", id, loss_scale)], inputs[(\"color\", 0, base)]))\n",
        "            reproj_losses = torch.cat(reproj_losses, 1)\n",
        "\n",
        "            identity_loss = []\n",
        "            for id in self.frame_diffs[1:]:\n",
        "                identity_loss.append(self.reprojection(inputs[(\"color\", id, base)], inputs[(\"color\", 0, base)]))\n",
        "            identity_loss = torch.cat(identity_loss, 1)\n",
        "\n",
        "\n",
        "            # Add some minor noise to ensure no repeated values\n",
        "            ## REDO THIS WITH NUMPY\n",
        "            identity_loss += torch.rand(identity_loss.shape).cuda() * 0.00001\n",
        "            total = torch.cat((identity_loss, reproj_losses), dim=1)\n",
        "            # Check if this is always executed\n",
        "            val, idxs = torch.min(total, dim=1)\n",
        "\n",
        "            # UNDERSTAND THIS LINE MORE\n",
        "            outputs[\"identity_selection/{}\".format(loss_scale)] = (idxs > identity_loss.shape[1] - 1).float()\n",
        "\n",
        "            current_loss += val.mean()\n",
        "\n",
        "            mean_disp = outputs[(\"disp\", loss_scale)].mean(2, True).mean(3, True)\n",
        "            norm_disp = outputs[(\"disp\", loss_scale)] / (mean_disp + 1e-7)\n",
        "            smooth_loss = monodepth2.layers.get_smooth_loss(norm_disp, inputs[(\"color\", 0, loss_scale)])\n",
        "\n",
        "            current_loss += 1e-3 * smooth_loss / (2 ** loss_scale)\n",
        "            complete_losses += current_loss\n",
        "            batch_loss[\"loss/{}\".format(loss_scale)] = current_loss\n",
        "\n",
        "        complete_losses /= len(self.loss_scales)\n",
        "        batch_loss[\"loss\"] = complete_losses\n",
        "        return batch_loss\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgYHittywMt5"
      },
      "source": [
        "import os\r\n",
        "import torchvision.models as models\r\n",
        "# def save_model(train):\r\n",
        "#     \"\"\"Save model weights to disk\r\n",
        "#     \"\"\"\r\n",
        "#     save_folder = os.path.join(\"/content/test_model/\")\r\n",
        "#     if not os.path.exists(save_folder):\r\n",
        "#         os.makedirs(save_folder)\r\n",
        "\r\n",
        "#     save_path_de = os.path.join(save_folder, \"depth_encoder.pth\")\r\n",
        "#     de = train.depth_encoder_network.state_dict()\r\n",
        "#     de['height'] = train.h\r\n",
        "#     de['width'] = train.w\r\n",
        "#     de['use_stereo'] = True    \r\n",
        "#     torch.save(de, save_path_de)\r\n",
        "    \r\n",
        "#     save_path_dd = os.path.join(save_folder, \"depth_decoder.pth\")\r\n",
        "#     torch.save(train.depth_decoder_network.state_dict(), save_path_dd)\r\n",
        "    \r\n",
        "#     save_path_pe = os.path.join(save_folder, \"pose_encoder.pth\")\r\n",
        "#     pe = train.pose_encoder_network.state_dict()\r\n",
        "#     pe['height'] = train.h\r\n",
        "#     pe['width'] = train.w\r\n",
        "#     pe['use_stereo'] = True    \r\n",
        "#     torch.save(pe, save_path_pe)\r\n",
        "\r\n",
        "#     save_path_pd = os.path.join(save_folder, \"pose_decoder.pth\")\r\n",
        "#     torch.save(train.pose_decoder_network.state_dict(), save_path_pd)\r\n",
        "#     to_save = train.pose_encoder_network.state_dict()\r\n",
        "\r\n",
        "#     save_path = os.path.join(save_folder, \"{}.pth\".format(\"adam\"))\r\n",
        "#     torch.save(train.adam_optim.state_dict(), save_path)\r\n",
        "\r\n",
        "# save_model(train)\r\n",
        "\r\n",
        "def save_model(train):\r\n",
        "    \"\"\"Save model weights to disk\r\n",
        "    \"\"\"\r\n",
        "    save_folder = os.path.join(\"/content/test_model/\")\r\n",
        "    if not os.path.exists(save_folder):\r\n",
        "        os.makedirs(save_folder)\r\n",
        "\r\n",
        "    for model_name, model in train.models.items():\r\n",
        "        save_path = os.path.join(save_folder, \"{}.pth\".format(model_name))\r\n",
        "        to_save = model.state_dict()\r\n",
        "        if 'encoder' in model_name:\r\n",
        "            # save the sizes - these are needed at prediction time\r\n",
        "            to_save['height'] = train.h\r\n",
        "            to_save['width'] = train.w\r\n",
        "            to_save['use_stereo'] = True\r\n",
        "        torch.save(to_save, save_path)\r\n",
        "\r\n",
        "    save_path = os.path.join(save_folder, \"{}.pth\".format(\"adam\"))\r\n",
        "    torch.save(train.adam_optim.state_dict(), save_path)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ow5DP4v9zQEH",
        "outputId": "aa7b0b7c-558d-40d9-868f-eaa541268e5f"
      },
      "source": [
        "train = MyTraining()\r\n",
        "train.run_training_loop()\r\n",
        "save_model(train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3385: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVMqZmbOPPeE"
      },
      "source": [
        "model_path = os.path.join(\"/content/test_model/\")\r\n"
      ],
      "execution_count": 25,
      "outputs": []
    }
  ]
}