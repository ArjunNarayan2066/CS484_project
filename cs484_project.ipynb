{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cs484_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1maJgn9Dp7KSMr0Vla80dQFTvxXxRJOLj",
      "authorship_tag": "ABX9TyOe4WkHQlDu4yVyzJ7V8+TL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArjunNarayan2066/CS484_project/blob/main/cs484_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "Self-supervised depth estimation consists on training a neural network to provide dense pixel-wise depth predictions given a single image. \"Self-supervision\"  in the training process implies not relying on ground truth depth maps, which can be hard and expensive to acquire. This project is motivated by Digging Into Self-Supervised Monocular Depth Estimation [1].\n",
        "\n",
        "The network attempts to generate a synthetic view of the same scene from a different point of view. This implicitly generated a disparity map, if per-pixel correspondences are known. The relative pose of the virtual viewpoint with respect to the actual viewpoint (i.e. real camera) is represented as $T_{t' \\rightarrow t}$, and the synthetic image is denoted $I_{t'}$, given an original image $I_t$.\n",
        "\n",
        "The loss functions employed in the training are as follows. \n",
        "$$L_p = \\Sigma_{t'} pe( I_{t}, I_{t'\\rightarrow t})$$\n",
        "\n",
        "where $pe$ represents the $L1$ distance between matching pixels in pixel space and is defined as:\n",
        "\n",
        "$$pe(I_a, I_b) = \\frac{\\alpha}{2}(1-SSIM(I_a, I_b)) + (1-\\alpha) ||I_a - I_b||_1$$\n",
        "\n",
        "$\\alpha$ is set to 0.85.\n",
        "\n",
        "$SSIM$ represents structural similarity between images $I_a$ and $I_b$. That is, the sum of x and y distances for all matches:\n",
        "\n",
        "$$SSIM(x, y) = \\frac{(2\\mu_x\\mu_y+c_1)(2\\sigma_{xy}+c_2)}{(\\mu_x^2+\\mu_y^2+c_1)(\\sigma_x^2+\\sigma_y^2+c_2)}$$\n",
        "\n",
        "$I_{t'\\rightarrow t}$ is defined as follows:\n",
        "\n",
        "$$I_{t'\\rightarrow t} = I_{t'}\\langle proj(D_t, T_{t\\rightarrow t'}, K)\\rangle$$\n",
        "\n",
        "for $D_t$ being projected depths, $\\langle . \\rangle$ being the sampling operator.\n",
        "\n",
        "Note that the $pe$ function provides a weighted sum of visually perceptible differences between images (SSIM), and the L1 norm between said images, as described in [2].\n",
        "\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnRlwbQ9z4kL",
        "outputId": "ad36e8f0-ac3b-4fce-90ff-462feb273ebd"
      },
      "source": [
        "! nvcc --version\n",
        "\n",
        "! pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "! pip install tensorboardX==1.4\n",
        "! pip install opencv-python==3.3.1.11\n",
        "! pip install kornia\n",
        "\n",
        "# Clone repo\n",
        "! git clone https://github.com/ArjunNarayan2066/monodepth2.gitpth2.git"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.7.1+cu101 in /usr/local/lib/python3.6/dist-packages (1.7.1+cu101)\n",
            "Requirement already satisfied: torchvision==0.8.2+cu101 in /usr/local/lib/python3.6/dist-packages (0.8.2+cu101)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.8.2+cu101) (7.0.0)\n",
            "Requirement already satisfied: tensorboardX==1.4 in /usr/local/lib/python3.6/dist-packages (1.4)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.4) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.4) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.4) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX==1.4) (50.3.2)\n",
            "Requirement already satisfied: opencv-python==3.3.1.11 in /usr/local/lib/python3.6/dist-packages (3.3.1.11)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python==3.3.1.11) (1.18.5)\n",
            "fatal: destination path 'monodepth2' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AK6si-eI1Owv"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import PIL.Image as pil\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import kornia\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "import networks\n",
        "from utils import download_model_if_doesnt_exist\n",
        "h"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'kornia'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-1-b2cd50c17556>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkornia\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'kornia'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyVajA6BA8CM"
      },
      "source": [
        "class SSIM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SSIM, self).__init__()\n",
        "        self.mu_x_pool   = nn.AvgPool2d(3, 1)\n",
        "        self.mu_y_pool   = nn.AvgPool2d(3, 1)\n",
        "        self.sig_x_pool  = nn.AvgPool2d(3, 1)\n",
        "        self.sig_y_pool  = nn.AvgPool2d(3, 1)\n",
        "        self.sig_xy_pool = nn.AvgPool2d(3, 1)\n",
        "\n",
        "        self.refl = nn.ReflectionPad2d(1)\n",
        "\n",
        "        self.C1 = 0.01 ** 2\n",
        "        self.C2 = 0.03 ** 2\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = self.refl(x)\n",
        "        y = self.refl(y)\n",
        "\n",
        "        mu_x = self.mu_x_pool(x)\n",
        "        mu_y = self.mu_y_pool(y)\n",
        "\n",
        "        sigma_x  = self.sig_x_pool(x ** 2) - mu_x ** 2\n",
        "        sigma_y  = self.sig_y_pool(y ** 2) - mu_y ** 2\n",
        "        sigma_xy = self.sig_xy_pool(x * y) - mu_x * mu_y\n",
        "\n",
        "        SSIM_n = (2 * mu_x * mu_y + self.C1) * (2 * sigma_xy + self.C2)\n",
        "        SSIM_d = (mu_x ** 2 + mu_y ** 2 + self.C1) * (sigma_x + sigma_y + self.C2)\n",
        "\n",
        "        return torch.clamp((1 - SSIM_n / SSIM_d) / 2, 0, 1)a\r\n"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! wget -i kitti_archives_to_download.txt -P kitti_data/\n",
        "\n",
        "! unzip -o \"kitti_data/*.zip\" -d \"kitti_data/\"\n",
        "\n",
        "# Convert to jpg\n",
        "! sudo apt update\n",
        "! sudo apt install imagemagick\n",
        "! sudo apt install parallel\n",
        "! find kitti_data/ -name '*.png' | parallel 'convert -quality 92 -sampling-factor 2x2,1x1,1x1 {.}.png {.}.jpg && rm {}'\n"
      ]
    },
    {
      "source": [
        "Generate a list of all the downloaded files, append $l$ or $r$ corresponding to which one of the cameras in the stereo pair it was taken on.\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! find . -type f -name '*.jpg' >> all_files.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fp = open('./all_files.txt', 'r')\n",
        "all_lines = fp.readlines()\n",
        "all_lines = [x.strip() for x in all_lines]\n",
        "\n",
        "\n",
        "##########################################################\n",
        "fp2 = open('./eigen_all_files.txt', 'r')\n",
        "eigen_lines = fp2.readlines()\n",
        "eigen_lines = [x.strip() for x in eigen_lines]\n",
        "\n",
        "output_file = open(\"output_files_train.txt\", \"w\")\n",
        "\n",
        "\n",
        "eigen_dict = {}\n",
        "\n",
        "for line in eigen_lines:\n",
        "    filepath, number, RL = line.split(' ')\n",
        "    number = str(int(number))\n",
        "\n",
        "    key = ' '.join([filepath, number])\n",
        "\n",
        "    eigen_dict[key] = RL\n",
        "\n",
        "for i, _ in enumerate(all_lines):\n",
        "    first_part = '/'.join( all_lines[i].split('/')[2:-3] )\n",
        "    second_part = str(int(all_lines[i].split('/')[-1].split('.')[0])) #remove trailing zeros\n",
        "\n",
        "    joined = ' '.join([first_part, second_part])\n",
        "    \n",
        "    if joined in eigen_dict:\n",
        "        output_file.write(' '.join([joined, eigen_dict[joined]]))\n",
        "        output_file.write('\\n')\n",
        "\n",
        "output_file.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsVkfx9I77aM",
        "outputId": "eb28bbd8-cd13-444c-d371-e631e7260e2d"
      },
      "source": [
        "import torch\r\n",
        "! uname -a\r\n",
        "print(torch.cuda.is_available())\r\n",
        "print(torch.cuda.get_device_name())"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linux 8f8e2609b2c4 4.19.112+ #1 SMP Thu Jul 23 08:00:38 PDT 2020 x86_64 x86_64 x86_64 GNU/Linux\n",
            "True\n",
            "Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRh0z4t5QoiJ"
      },
      "source": [
        "# ! python monodepth2/train.py --model_name stereo_model  --frame_ids 0 --use_stereo --split eigen_full"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LaW3qupTEJS"
      },
      "source": [
        "# ! find kitti_data/** -name '*.png' | parallel 'convert -quality 92 -sampling-factor 2x2,1x1,1x1 {.}.png {.}.jpg && rm {}'"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuIvPhbqecxe"
      },
      "source": [
        "# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_28_calib.zip\r\n",
        "# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_28_drive_0001/2011_09_28_drive_0001_sync.zip\r\n",
        "# ! wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_28_drive_0002/2011_09_28_drive_0002_sync.zip\r\n",
        "\r\n"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXHNpu6ySrUT"
      },
      "source": [
        "# ! rm -rf kitti_data\r\n",
        "# ! mkdir kitti_data\r\n",
        "# ! unzip -q 2011_09_28_drive_0001_sync.zip -d kitti_data\r\n",
        "# ! rm -rf 2011_09_28_drive_0001_sync.zip\r\n",
        "# ! mv data_temp/2011_09_26/2011_09_26_drive_0095_sync/* kitti_data"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5JE0YgXjVe-"
      },
      "source": [
        "# ! sudo apt update\r\n",
        "# ! sudo apt install imagemagick --fix-missing\r\n",
        "# ! convert -h\r\n",
        "# ! find kitti_data/ -name '*.png'\r\n",
        "# ! sudo apt install parallel\r\n",
        "# convert -quality 92 -sampling-factor 2x2,1x1,1x1 kitti_data/2011_09_26/2011_09_26_drive_0048_sync/image_02/data/0000000005.png jpg && rm {}\r\n",
        "# ! find kitti_data/2011_09_28 -name '*.png' | parallel 'convert -quality 92 -sampling-factor 2x2,1x1,1x1 {.}.png {.}.jpg && rm {}'"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5yRZNzAj_gx",
        "outputId": "b94e4d8c-a690-4ead-8d94-0d16e06276ea"
      },
      "source": [
        "! python monodepth2/train.py --model_name S_640x192 --frame_ids 0 --use_stereo --pose_model_type separate_resnet --split eigen_full --data_path /content/kitti_data --num_epochs 10\r\n",
        "# /content/kitti_data/2011_09_26/2011_09_26_drive_0106_sync/image_02/data/0000000115.png"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "Training model named:\n",
            "   S_640x192\n",
            "Models and tensorboard events files are saved to:\n",
            "   /root/tmp\n",
            "Training is using:\n",
            "   cuda\n",
            "Using split:\n",
            "   eigen_full\n",
            "There are 670 training items and 82 validation items\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "Training\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3385: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n",
            "epoch   0 | batch      0 | examples/s:   4.4 | loss: 0.21596 | time elapsed: 00h00m32s | time left: 00h00m00s\n",
            "Training\n",
            "epoch   1 | batch      0 | examples/s:   7.6 | loss: 0.15104 | time elapsed: 00h02m19s | time left: 00h20m58s\n",
            "Training\n",
            "epoch   2 | batch      0 | examples/s:   7.1 | loss: 0.13181 | time elapsed: 00h04m06s | time left: 00h16m27s\n",
            "Training\n",
            "epoch   3 | batch      0 | examples/s:   7.3 | loss: 0.13445 | time elapsed: 00h05m52s | time left: 00h13m42s\n",
            "Training\n",
            "Traceback (most recent call last):\n",
            "  File \"monodepth2/train.py\", line 18, in <module>\n",
            "    trainer.train()\n",
            "  File \"/content/monodepth2/trainer.py\", line 190, in train\n",
            "    self.run_epoch()\n",
            "  File \"/content/monodepth2/trainer.py\", line 202, in run_epoch\n",
            "    for batch_idx, inputs in enumerate(self.train_loader):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 435, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1068, in _next_data\n",
            "    idx, data = self._get_data()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1024, in _get_data\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 872, in _try_get_data\n",
            "    data = self._data_queue.get(timeout=timeout)\n",
            "  File \"/usr/lib/python3.6/queue.py\", line 173, in get\n",
            "    self.not_empty.wait(remaining)\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 299, in wait\n",
            "    gotit = waiter.acquire(True, timeout)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLRxLc8i0-ss"
      },
      "source": [
        "# ! zip -q -r /content/model.zip /root/tmp/S_640x192/*\r\n",
        "# ! ls -la /root/tmp/S_640x192/models/"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2l-qT82snuo"
      },
      "source": [
        "# ! python monodepth2/export_gt_depth.py --data_path kitti_data --split eigen"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWvl5VeiuppM"
      },
      "source": [
        "# ! python monodepth2/evaluate_depth.py --data_path kitti_data --load_weights_folder /root/tmp/S_640x192/models/weights_19/ --eval_stereo"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prwQU_TPyWLf"
      },
      "source": [
        "#@title NETWORKS\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from collections import OrderedDict\n",
        "\n",
        "class Conv3x3(nn.Module):\n",
        "    \"\"\"Layer to pad and convolve input\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, use_refl=True):\n",
        "        super(Conv3x3, self).__init__()\n",
        "\n",
        "        if use_refl:\n",
        "            self.pad = nn.ReflectionPad2d(1)\n",
        "        else:\n",
        "            self.pad = nn.ZeroPad2d(1)\n",
        "        self.conv = nn.Conv2d(int(in_channels), int(out_channels), 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pad(x)\n",
        "        out = self.conv(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"Layer to perform a convolution followed by ELU\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ConvBlock, self).__init__()\n",
        "\n",
        "        self.conv = Conv3x3(in_channels, out_channels)\n",
        "        self.nonlin = nn.ELU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.nonlin(out)\n",
        "        return out\n",
        "\n",
        "class ResnetEncoder(nn.Module):\n",
        "    \"\"\"Pytorch module for a resnet encoder\n",
        "    \"\"\"\n",
        "    def __init__(self, num_layers, pretrained, num_input_images=1):\n",
        "        super(ResnetEncoder, self).__init__()\n",
        "\n",
        "        self.num_ch_enc = np.array([64, 64, 128, 256, 512])\n",
        "\n",
        "        resnets = {18: models.resnet18,\n",
        "                   34: models.resnet34,\n",
        "                   50: models.resnet50,\n",
        "                   101: models.resnet101,\n",
        "                   152: models.resnet152}\n",
        "\n",
        "        if num_layers not in resnets:\n",
        "            raise ValueError(\"{} is not a valid number of resnet layers\".format(num_layers))\n",
        "\n",
        "        if num_input_images > 1:\n",
        "            self.encoder = resnet_multiimage_input(num_layers, pretrained, num_input_images)\n",
        "        else:\n",
        "            self.encoder = resnets[num_layers](pretrained)\n",
        "\n",
        "        if num_layers > 34:\n",
        "            self.num_ch_enc[1:] *= 4\n",
        "\n",
        "    def forward(self, input_image):\n",
        "        self.features = []\n",
        "        x = (input_image - 0.45) / 0.225\n",
        "        x = self.encoder.conv1(x)\n",
        "        x = self.encoder.bn1(x)\n",
        "        self.features.append(self.encoder.relu(x))\n",
        "        self.features.append(self.encoder.layer1(self.encoder.maxpool(self.features[-1])))\n",
        "        self.features.append(self.encoder.layer2(self.features[-1]))\n",
        "        self.features.append(self.encoder.layer3(self.features[-1]))\n",
        "        self.features.append(self.encoder.layer4(self.features[-1]))\n",
        "\n",
        "        return self.features\n",
        "\n",
        "class DepthDecoder(nn.Module):\n",
        "    def __init__(self, num_ch_enc, scales=range(4), num_output_channels=1, use_skips=True):\n",
        "        super(DepthDecoder, self).__init__()\n",
        "\n",
        "        self.num_output_channels = num_output_channels\n",
        "        self.use_skips = use_skips\n",
        "        self.upsample_mode = 'nearest'\n",
        "        self.scales = scales\n",
        "\n",
        "        self.num_ch_enc = num_ch_enc\n",
        "        self.num_ch_dec = np.array([16, 32, 64, 128, 256])\n",
        "\n",
        "        # decoder\n",
        "        self.convs = OrderedDict()\n",
        "        for i in range(4, -1, -1):\n",
        "            # upconv_0\n",
        "            num_ch_in = self.num_ch_enc[-1] if i == 4 else self.num_ch_dec[i + 1]\n",
        "            num_ch_out = self.num_ch_dec[i]\n",
        "            self.convs[(\"upconv\", i, 0)] = ConvBlock(num_ch_in, num_ch_out)\n",
        "\n",
        "            # upconv_1\n",
        "            num_ch_in = self.num_ch_dec[i]\n",
        "            if self.use_skips and i > 0:\n",
        "                num_ch_in += self.num_ch_enc[i - 1]\n",
        "            num_ch_out = self.num_ch_dec[i]\n",
        "            self.convs[(\"upconv\", i, 1)] = ConvBlock(num_ch_in, num_ch_out)\n",
        "\n",
        "        for s in self.scales:\n",
        "            self.convs[(\"dispconv\", s)] = Conv3x3(self.num_ch_dec[s], self.num_output_channels)\n",
        "\n",
        "        self.decoder = nn.ModuleList(list(self.convs.values()))\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_features):\n",
        "        self.outputs = {}\n",
        "\n",
        "        # decoder\n",
        "        x = input_features[-1]\n",
        "        for i in range(4, -1, -1):\n",
        "            x = self.convs[(\"upconv\", i, 0)](x)\n",
        "            x = [upsample(x)]\n",
        "            if self.use_skips and i > 0:\n",
        "                x += [input_features[i - 1]]\n",
        "            x = torch.cat(x, 1)\n",
        "            x = self.convs[(\"upconv\", i, 1)](x)\n",
        "            if i in self.scales:\n",
        "                self.outputs[(\"disp\", i)] = self.sigmoid(self.convs[(\"dispconv\", i)](x))\n",
        "\n",
        "        return self.outputs\n",
        "\n",
        "class PoseCNN(nn.Module):\n",
        "    def __init__(self, frame_count):\n",
        "        super(PoseCNN, self).__init__()\n",
        "\n",
        "        self.frame_count = frame_count\n",
        "        self.final_pose_cnn = nn.Conv2d(256, 6*(self.frame_count-1),1)\n",
        "        self.relu = nn.ReLU(True)\n",
        "        self.conv_count = 7\n",
        "\n",
        "        self.conv_1 = nn.Conv2d(3*self.frame_count, 16, kernel_size=7, stride=2, padding=3)\n",
        "        self.conv_2 = nn.Conv2d(16, 32,   kernel_size=5, stride=2, padding=2)\n",
        "        self.conv_3 = nn.Conv2d(32, 64,   kernel_size=3, stride=2, padding=1)\n",
        "        self.conv_4 = nn.Conv2d(64, 128,  kernel_size=3, stride=2, padding=1)\n",
        "        self.conv_5 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv_6 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv_7 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.net = nn.ModuleList([self.conv_1, self.conv_2, self.conv_3, \n",
        "                                  self.conv_4, self.conv_5, self.conv_6, \n",
        "                                  self.conv_7])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for cnn in self.net:\n",
        "            x = cnn(x)\n",
        "            x = self.relu(x)\n",
        "\n",
        "        x = self.final_pose_cnn(x)\n",
        "        x = x.mean(3)\n",
        "        x = x.mean(2)\n",
        "\n",
        "        x = x.view(-1, self.frame_count-1, 1, 6)\n",
        "        x /= 100.0\n",
        "\n",
        "        all_axis_angles = x[:,:,:,:3]\n",
        "        all_translation_coords = x[:,:,:,3:]\n",
        "\n",
        "        return all_axis_angles, all_translation_coords\n",
        "\n",
        "\n"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOnW16nVP0Fv"
      },
      "source": [
        "import os\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "import copy\r\n",
        "from PIL import Image  # using pillow-simd for increased speed\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.utils.data as data\r\n",
        "from torchvision import transforms\r\n",
        "\r\n",
        "class BackprojectDepth(nn.Module):\r\n",
        "    \"\"\"Layer to transform a depth image into a point cloud\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, batch_size, height, width):\r\n",
        "        super(BackprojectDepth, self).__init__()\r\n",
        "\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.height = height\r\n",
        "        self.width = width\r\n",
        "\r\n",
        "        meshgrid = np.meshgrid(range(self.width), range(self.height), indexing='xy')\r\n",
        "        self.id_coords = np.stack(meshgrid, axis=0).astype(np.float32)\r\n",
        "        self.id_coords = nn.Parameter(torch.from_numpy(self.id_coords),\r\n",
        "                                      requires_grad=False)\r\n",
        "\r\n",
        "        self.ones = nn.Parameter(torch.ones(self.batch_size, 1, self.height * self.width),\r\n",
        "                                 requires_grad=False)\r\n",
        "\r\n",
        "        self.pix_coords = torch.unsqueeze(torch.stack(\r\n",
        "            [self.id_coords[0].view(-1), self.id_coords[1].view(-1)], 0), 0)\r\n",
        "        self.pix_coords = self.pix_coords.repeat(batch_size, 1, 1)\r\n",
        "        self.pix_coords = nn.Parameter(torch.cat([self.pix_coords, self.ones], 1),\r\n",
        "                                       requires_grad=False)\r\n",
        "\r\n",
        "    def forward(self, depth, inv_K):\r\n",
        "        cam_points = torch.matmul(inv_K[:, :3, :3], self.pix_coords)\r\n",
        "        cam_points = depth.view(self.batch_size, 1, -1) * cam_points\r\n",
        "        cam_points = torch.cat([cam_points, self.ones], 1)\r\n",
        "\r\n",
        "        return cam_points\r\n",
        "\r\n",
        "\r\n",
        "class Project3D(nn.Module):\r\n",
        "    \"\"\"Layer which projects 3D points into a camera with intrinsics K and at position T\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, batch_size, height, width, eps=1e-7):\r\n",
        "        super(Project3D, self).__init__()\r\n",
        "\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.height = height\r\n",
        "        self.width = width\r\n",
        "        self.eps = eps\r\n",
        "\r\n",
        "    def forward(self, points, K, T):\r\n",
        "        P = torch.matmul(K, T)[:, :3, :]\r\n",
        "\r\n",
        "        cam_points = torch.matmul(P, points)\r\n",
        "\r\n",
        "        pix_coords = cam_points[:, :2, :] / (cam_points[:, 2, :].unsqueeze(1) + self.eps)\r\n",
        "        pix_coords = pix_coords.view(self.batch_size, 2, self.height, self.width)\r\n",
        "        pix_coords = pix_coords.permute(0, 2, 3, 1)\r\n",
        "        pix_coords[..., 0] /= self.width - 1\r\n",
        "        pix_coords[..., 1] /= self.height - 1\r\n",
        "        pix_coords = (pix_coords - 0.5) * 2\r\n",
        "        return pix_coords\r\n",
        "\r\n",
        "def pil_loader(path):\r\n",
        "    # open path as file to avoid ResourceWarning\r\n",
        "    # (https://github.com/python-pillow/Pillow/issues/835)\r\n",
        "    with open(path, 'rb') as f:\r\n",
        "        with Image.open(f) as img:\r\n",
        "            return img.convert('RGB')\r\n",
        "\r\n",
        "\r\n",
        "class MonoDataset(data.Dataset):\r\n",
        "    \"\"\"Superclass for monocular dataloaders\r\n",
        "\r\n",
        "    Args:\r\n",
        "        data_path\r\n",
        "        filenames\r\n",
        "        height\r\n",
        "        width\r\n",
        "        frame_idxs\r\n",
        "        num_scales\r\n",
        "        is_train\r\n",
        "        img_ext\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self,\r\n",
        "                 data_path,\r\n",
        "                 filenames,\r\n",
        "                 height,\r\n",
        "                 width,\r\n",
        "                 frame_idxs,\r\n",
        "                 num_scales,\r\n",
        "                 is_train=False,\r\n",
        "                 img_ext='.jpg'):\r\n",
        "        super(MonoDataset, self).__init__()\r\n",
        "\r\n",
        "        self.data_path = data_path\r\n",
        "        self.filenames = filenames\r\n",
        "        self.height = height\r\n",
        "        self.width = width\r\n",
        "        self.num_scales = num_scales\r\n",
        "        self.interp = Image.ANTIALIAS\r\n",
        "\r\n",
        "        self.frame_idxs = frame_idxs\r\n",
        "\r\n",
        "        self.is_train = is_train\r\n",
        "        self.img_ext = img_ext\r\n",
        "\r\n",
        "        self.loader = pil_loader\r\n",
        "        self.to_tensor = transforms.ToTensor()\r\n",
        "\r\n",
        "        # We need to specify augmentations differently in newer versions of torchvision.\r\n",
        "        # We first try the newer tuple version; if this fails we fall back to scalars\r\n",
        "        try:\r\n",
        "            self.brightness = (0.8, 1.2)\r\n",
        "            self.contrast = (0.8, 1.2)\r\n",
        "            self.saturation = (0.8, 1.2)\r\n",
        "            self.hue = (-0.1, 0.1)\r\n",
        "            transforms.ColorJitter.get_params(\r\n",
        "                self.brightness, self.contrast, self.saturation, self.hue)\r\n",
        "        except TypeError:\r\n",
        "            self.brightness = 0.2\r\n",
        "            self.contrast = 0.2\r\n",
        "            self.saturation = 0.2\r\n",
        "            self.hue = 0.1\r\n",
        "\r\n",
        "        self.resize = {}\r\n",
        "        for i in range(self.num_scales):\r\n",
        "            s = 2 ** i\r\n",
        "            self.resize[i] = transforms.Resize((self.height // s, self.width // s),\r\n",
        "                                               interpolation=self.interp)\r\n",
        "\r\n",
        "        self.load_depth = self.check_depth()\r\n",
        "\r\n",
        "    def preprocess(self, inputs, color_aug):\r\n",
        "        \"\"\"Resize colour images to the required scales and augment if required\r\n",
        "\r\n",
        "        We create the color_aug object in advance and apply the same augmentation to all\r\n",
        "        images in this item. This ensures that all images input to the pose network receive the\r\n",
        "        same augmentation.\r\n",
        "        \"\"\"\r\n",
        "        for k in list(inputs):\r\n",
        "            frame = inputs[k]\r\n",
        "            if \"color\" in k:\r\n",
        "                n, im, i = k\r\n",
        "                for i in range(self.num_scales):\r\n",
        "                    inputs[(n, im, i)] = self.resize[i](inputs[(n, im, i - 1)])\r\n",
        "\r\n",
        "        for k in list(inputs):\r\n",
        "            f = inputs[k]\r\n",
        "            if \"color\" in k:\r\n",
        "                n, im, i = k\r\n",
        "                inputs[(n, im, i)] = self.to_tensor(f)\r\n",
        "                inputs[(n + \"_aug\", im, i)] = self.to_tensor(color_aug(f))\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.filenames)\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        \"\"\"Returns a single training item from the dataset as a dictionary.\r\n",
        "\r\n",
        "        Values correspond to torch tensors.\r\n",
        "        Keys in the dictionary are either strings or tuples:\r\n",
        "\r\n",
        "            (\"color\", <frame_id>, <scale>)          for raw colour images,\r\n",
        "            (\"color_aug\", <frame_id>, <scale>)      for augmented colour images,\r\n",
        "            (\"K\", scale) or (\"inv_K\", scale)        for camera intrinsics,\r\n",
        "            \"stereo_T\"                              for camera extrinsics, and\r\n",
        "            \"depth_gt\"                              for ground truth depth maps.\r\n",
        "\r\n",
        "        <frame_id> is either:\r\n",
        "            an integer (e.g. 0, -1, or 1) representing the temporal step relative to 'index',\r\n",
        "        or\r\n",
        "            \"s\" for the opposite image in the stereo pair.\r\n",
        "\r\n",
        "        <scale> is an integer representing the scale of the image relative to the fullsize image:\r\n",
        "            -1      images at native resolution as loaded from disk\r\n",
        "            0       images resized to (self.width,      self.height     )\r\n",
        "            1       images resized to (self.width // 2, self.height // 2)\r\n",
        "            2       images resized to (self.width // 4, self.height // 4)\r\n",
        "            3       images resized to (self.width // 8, self.height // 8)\r\n",
        "        \"\"\"\r\n",
        "        inputs = {}\r\n",
        "\r\n",
        "        do_color_aug = self.is_train and random.random() > 0.5\r\n",
        "        do_flip = self.is_train and random.random() > 0.5\r\n",
        "\r\n",
        "        line = self.filenames[index].split()\r\n",
        "        folder = line[0]\r\n",
        "\r\n",
        "        if len(line) == 3:\r\n",
        "            frame_index = int(line[1])\r\n",
        "        else:\r\n",
        "            frame_index = 0\r\n",
        "\r\n",
        "        if len(line) == 3:\r\n",
        "            side = line[2]\r\n",
        "        else:\r\n",
        "            side = None\r\n",
        "\r\n",
        "        for i in self.frame_idxs:\r\n",
        "            if i == \"s\":\r\n",
        "                other_side = {\"r\": \"l\", \"l\": \"r\"}[side]\r\n",
        "                inputs[(\"color\", i, -1)] = self.get_color(folder, frame_index, other_side, do_flip)\r\n",
        "            else:\r\n",
        "                inputs[(\"color\", i, -1)] = self.get_color(folder, frame_index + i, side, do_flip)\r\n",
        "\r\n",
        "        # adjusting intrinsics to match each scale in the pyramid\r\n",
        "        for scale in range(self.num_scales):\r\n",
        "            K = self.K.copy()\r\n",
        "\r\n",
        "            K[0, :] *= self.width // (2 ** scale)\r\n",
        "            K[1, :] *= self.height // (2 ** scale)\r\n",
        "\r\n",
        "            inv_K = np.linalg.pinv(K)\r\n",
        "\r\n",
        "            inputs[(\"K\", scale)] = torch.from_numpy(K)\r\n",
        "            inputs[(\"inv_K\", scale)] = torch.from_numpy(inv_K)\r\n",
        "\r\n",
        "        if do_color_aug:\r\n",
        "            color_aug = transforms.ColorJitter.get_params(\r\n",
        "                self.brightness, self.contrast, self.saturation, self.hue)\r\n",
        "        else:\r\n",
        "            color_aug = (lambda x: x)\r\n",
        "\r\n",
        "        self.preprocess(inputs, color_aug)\r\n",
        "\r\n",
        "        for i in self.frame_idxs:\r\n",
        "            del inputs[(\"color\", i, -1)]\r\n",
        "            del inputs[(\"color_aug\", i, -1)]\r\n",
        "\r\n",
        "        if self.load_depth:\r\n",
        "            depth_gt = self.get_depth(folder, frame_index, side, do_flip)\r\n",
        "            inputs[\"depth_gt\"] = np.expand_dims(depth_gt, 0)\r\n",
        "            inputs[\"depth_gt\"] = torch.from_numpy(inputs[\"depth_gt\"].astype(np.float32))\r\n",
        "\r\n",
        "        if \"s\" in self.frame_idxs:\r\n",
        "            stereo_T = np.eye(4, dtype=np.float32)\r\n",
        "            baseline_sign = -1 if do_flip else 1\r\n",
        "            side_sign = -1 if side == \"l\" else 1\r\n",
        "            stereo_T[0, 3] = side_sign * baseline_sign * 0.1\r\n",
        "\r\n",
        "            inputs[\"stereo_T\"] = torch.from_numpy(stereo_T)\r\n",
        "\r\n",
        "        return inputs\r\n",
        "\r\n",
        "    def get_color(self, folder, frame_index, side, do_flip):\r\n",
        "        raise NotImplementedError\r\n",
        "\r\n",
        "    def check_depth(self):\r\n",
        "        raise NotImplementedError\r\n",
        "\r\n",
        "    def get_depth(self, folder, frame_index, side, do_flip):\r\n",
        "        raise NotImplementedError\r\n",
        "\r\n",
        "class KITTIDataset(MonoDataset):\r\n",
        "    \"\"\"Superclass for different types of KITTI dataset loaders\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, *args, **kwargs):\r\n",
        "        super(KITTIDataset, self).__init__(*args, **kwargs)\r\n",
        "\r\n",
        "        # NOTE: Make sure your intrinsics matrix is *normalized* by the original image size    \r\n",
        "        self.K = np.array([[0.58, 0, 0.5, 0],\r\n",
        "                           [0, 1.92, 0.5, 0],\r\n",
        "                           [0, 0, 1, 0],\r\n",
        "                           [0, 0, 0, 1]], dtype=np.float32)\r\n",
        "\r\n",
        "        self.full_res_shape = (1242, 375)\r\n",
        "        self.side_map = {\"2\": 2, \"3\": 3, \"l\": 2, \"r\": 3}\r\n",
        "\r\n",
        "    def check_depth(self):\r\n",
        "        line = self.filenames[0].split()\r\n",
        "        scene_name = line[0]\r\n",
        "        frame_index = int(line[1])\r\n",
        "\r\n",
        "        velo_filename = os.path.join(\r\n",
        "            self.data_path,\r\n",
        "            scene_name,\r\n",
        "            \"velodyne_points/data/{:010d}.bin\".format(int(frame_index)))\r\n",
        "\r\n",
        "        return os.path.isfile(velo_filename)\r\n",
        "\r\n",
        "    def get_color(self, folder, frame_index, side, do_flip):\r\n",
        "        color = self.loader(self.get_image_path(folder, frame_index, side))\r\n",
        "\r\n",
        "        if do_flip:\r\n",
        "            color = color.transpose(pil.FLIP_LEFT_RIGHT)\r\n",
        "\r\n",
        "        return color\r\n",
        "\r\n",
        "\r\n",
        "class KITTIRAWDataset(KITTIDataset):\r\n",
        "    \"\"\"KITTI dataset which loads the original velodyne depth maps for ground truth\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, *args, **kwargs):\r\n",
        "        super(KITTIRAWDataset, self).__init__(*args, **kwargs)\r\n",
        "\r\n",
        "    def get_image_path(self, folder, frame_index, side):\r\n",
        "        f_str = \"{:010d}{}\".format(frame_index, self.img_ext)\r\n",
        "        image_path = os.path.join(\r\n",
        "            self.data_path, folder, \"image_0{}/data\".format(self.side_map[side]), f_str)\r\n",
        "        return image_path\r\n",
        "\r\n",
        "    def get_depth(self, folder, frame_index, side, do_flip):\r\n",
        "        calib_path = os.path.join(self.data_path, folder.split(\"/\")[0])\r\n",
        "\r\n",
        "        velo_filename = os.path.join(\r\n",
        "            self.data_path,\r\n",
        "            folder,\r\n",
        "            \"velodyne_points/data/{:010d}.bin\".format(int(frame_index)))\r\n",
        "\r\n",
        "        depth_gt = generate_depth_map(calib_path, velo_filename, self.side_map[side])\r\n",
        "        depth_gt = skimage.transform.resize(\r\n",
        "            depth_gt, self.full_res_shape[::-1], order=0, preserve_range=True, mode='constant')\r\n",
        "\r\n",
        "        if do_flip:\r\n",
        "            depth_gt = np.fliplr(depth_gt)\r\n",
        "\r\n",
        "        return depth_gt\r\n",
        "\r\n"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from collections import OrderedDict\n",
        "import layers\n",
        "from layers import *\n",
        "\n",
        "\n",
        "class DepthDecoder(nn.Module):\n",
        "    def __init__(self, num_ch_enc, num_channels_out=1):\n",
        "        super(DepthDecoder, self).__init__()\n",
        "\n",
        "        self.num_channels_out = num_channels_out\n",
        "        self.scales = [0,1,2,3]\n",
        "\n",
        "        self.num_ch_enc = num_ch_enc\n",
        "        self.num_ch_dec = np.array([16, 32, 64, 128, 256])\n",
        "\n",
        "        # decoder   \n",
        "        self.convs = OrderedDict()\n",
        "\n",
        "        ### 4 ###\n",
        "        # upconv_0\n",
        "        num_ch_in = self.num_ch_enc[-1]\n",
        "        num_ch_out = self.num_ch_dec[4]\n",
        "        self.convs[(\"upconv\", 4, 0)] = layers.ConvBlock(num_ch_in, num_ch_out)\n",
        "\n",
        "        # upconv_1\n",
        "        num_ch_in = self.num_ch_dec[4]\n",
        "        #skip connection\n",
        "        num_ch_in += self.num_ch_enc[3]\n",
        "        num_ch_out = self.num_ch_dec[4]\n",
        "        self.convs[(\"upconv\", 4, 1)] = layers.ConvBlock(num_ch_in, num_ch_out)\n",
        "\n",
        "        ### 3 ###\n",
        "        # upconv_0\n",
        "        num_ch_in = self.num_ch_dec[4]\n",
        "        num_ch_out = self.num_ch_dec[3]\n",
        "        self.convs[(\"upconv\", 3, 0)] = layers.ConvBlock(num_ch_in, num_ch_out)\n",
        "\n",
        "        # upconv_1\n",
        "        num_ch_in = self.num_ch_dec[3]\n",
        "        #skip connection\n",
        "        num_ch_in += self.num_ch_enc[2]\n",
        "        num_ch_out = self.num_ch_dec[3]\n",
        "        self.convs[(\"upconv\", 3, 1)] = layers.ConvBlock(num_ch_in, num_ch_out)\n",
        "\n",
        "        ### 2 ###\n",
        "        # upconv_0\n",
        "        num_ch_in = self.num_ch_dec[3]\n",
        "        num_ch_out = self.num_ch_dec[2]\n",
        "        self.convs[(\"upconv\", 2, 0)] = layers.ConvBlock(num_ch_in, num_ch_out)\n",
        "\n",
        "        # upconv_1\n",
        "        num_ch_in = self.num_ch_dec[2]\n",
        "        #skip connection\n",
        "        num_ch_in += self.num_ch_enc[1]\n",
        "        num_ch_out = self.num_ch_dec[2]\n",
        "        self.convs[(\"upconv\", 2, 1)] = layers.ConvBlock(num_ch_in, num_ch_out)\n",
        "\n",
        "        ### 1 ###\n",
        "        # upconv_0\n",
        "        num_ch_in = self.num_ch_dec[2]\n",
        "        num_ch_out = self.num_ch_dec[1]\n",
        "        self.convs[(\"upconv\", 1, 0)] = layers.ConvBlock(num_ch_in, num_ch_out)\n",
        "\n",
        "        # upconv_1\n",
        "        num_ch_in = self.num_ch_dec[1]\n",
        "        #skip connection\n",
        "        num_ch_in += self.num_ch_enc[0]\n",
        "        num_ch_out = self.num_ch_dec[1]\n",
        "        self.convs[(\"upconv\", 1, 1)] = layers.ConvBlock(num_ch_in, num_ch_out)\n",
        "\n",
        "        ### 0 ###\n",
        "        # upconv_0\n",
        "        num_ch_in = self.num_ch_dec[1]\n",
        "        num_ch_out = self.num_ch_dec[0]\n",
        "        self.convs[(\"upconv\", 0, 0)] = layers.ConvBlock(num_ch_in, num_ch_out)\n",
        "\n",
        "        # upconv_1\n",
        "        num_ch_in = self.num_ch_dec[0]\n",
        "        # No skip connection on the last layer\n",
        "        num_ch_out = self.num_ch_dec[0]\n",
        "        self.convs[(\"upconv\", 0, 1)] = layers.ConvBlock(num_ch_in, num_ch_out)\n",
        "\n",
        "\n",
        "\n",
        "        for s in self.scales:\n",
        "            self.convs[(\"dispconv\", s)] = layers.Conv3x3(self.num_ch_dec[s], self.num_channels_out)\n",
        "\n",
        "        self.decoder = nn.ModuleList(list(self.convs.values()))\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_features):\n",
        "        self.outputs = {}\n",
        "\n",
        "        x = input_features[-1]\n",
        "\n",
        "        ### 4 ###\n",
        "        x = self.convs[(\"upconv\", 4, 0)](x)\n",
        "        x = [upsample(x)]\n",
        "        x += [input_features[3]]\n",
        "        x = torch.cat(x, 1)\n",
        "        x = self.convs[(\"upconv\", 4, 1)](x)\n",
        "\n",
        "        ### 3 ###\n",
        "        x = self.convs[(\"upconv\", 3, 0)](x)\n",
        "        x = [upsample(x)]\n",
        "        x += [input_features[2]]\n",
        "        x = torch.cat(x, 1)\n",
        "        x = self.convs[(\"upconv\", 3, 1)](x)\n",
        "        self.outputs[(\"disp\", 3)] = self.sigmoid(self.convs[(\"dispconv\", 3)](x))\n",
        "\n",
        "        ### 2 ###\n",
        "        x = self.convs[(\"upconv\", 2, 0)](x)\n",
        "        x = [upsample(x)]\n",
        "        x += [input_features[1]]\n",
        "        x = torch.cat(x, 1)\n",
        "        x = self.convs[(\"upconv\", 2, 1)](x)\n",
        "        self.outputs[(\"disp\", 2)] = self.sigmoid(self.convs[(\"dispconv\", 2)](x))\n",
        "\n",
        "        ### 1 ###\n",
        "        x = self.convs[(\"upconv\", 1, 0)](x)\n",
        "        x = [upsample(x)]\n",
        "        x += [input_features[0]]\n",
        "        x = torch.cat(x, 1)\n",
        "        x = self.convs[(\"upconv\", 1, 1)](x)\n",
        "        self.outputs[(\"disp\", 1)] = self.sigmoid(self.convs[(\"dispconv\", 1)](x))\n",
        "\n",
        "        ### 0 ###\n",
        "        x = self.convs[(\"upconv\", 0, 0)](x)\n",
        "        x = [upsample(x)]\n",
        "        x = torch.cat(x, 1)\n",
        "        x = self.convs[(\"upconv\", 0, 1)](x)\n",
        "        self.outputs[(\"disp\", 0)] = self.sigmoid(self.convs[(\"dispconv\", 0)](x))\n",
        "\n",
        "        return self.outputs"
      ]
    },
    {
      "source": [
        "## Compute gradient of image to give Loss function different weights"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_gradient(t, gauss_blur_sigma=None, kernel_size = 5):\n",
        "  a = torch.Tensor([[1, 0, -1],\n",
        "  [2, 0, -2],\n",
        "  [1, 0, -1]])\n",
        "\n",
        "  a = a.view((1,1,3,3))\n",
        "  G_x = F.conv2d(t, a)\n",
        "\n",
        "  b = torch.Tensor([[1, 2, 1],\n",
        "  [0, 0, 0],\n",
        "  [-1, -2, -1]])\n",
        "\n",
        "  b = b.view((1,1,3,3))\n",
        "  G_y = F.conv2d(t, b)\n",
        "\n",
        "  G = torch.sqrt(torch.pow(G_x,2)+ torch.pow(G_y,2))\n",
        "\n",
        "\n",
        "  if gauss_blur_sigma:\n",
        "    G = kornia.gaussian_blur2d(G, (kernel_size, kernel_size), (gauss_blur_sigma, gauss_blur_sigma))\n",
        "\n",
        "  G -= torch.min(G)\n",
        "  G /= torch.max(G)\n",
        "\n",
        "\n",
        "  return G"
      ]
    },
    {
      "source": [
        "### Testing the gradient function:"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Read in image\n",
        "im = Image.open(\"./kitti_data/2011_09_28/2011_09_28_drive_0002_sync/image_00/data/0000000172.png\")\n",
        "\n",
        "# Convert to tensor and compute gradient\n",
        "t = transforms.ToTensor()\n",
        "t_im = t(im)[None]\n",
        "t_im.shape\n",
        "grad = get_gradient(t_im, gauss_blur_sigma=2)\n",
        "\n",
        "\n",
        "t2 = transforms.ToPILImage()\n",
        "g = t2(grad[0])\n",
        "\n",
        "g\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7mdtNn2xC6I"
      },
      "source": [
        "###\n",
        "###\n",
        "### Training code is based on a very-stripped down version of https://arxiv.org/pdf/1806.01260.pdf\n",
        "### Written only with stereo based training with reduced feature set\n",
        "###\n",
        "###\n",
        "\n",
        "\n",
        "import monodepth2.layers, monodepth2.utils, monodepth2.trainer\n",
        "import monodepth2.networks\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import timeit\n",
        "import PIL.Image as pil\n",
        "\n",
        "\n",
        "class MyTraining(object):\n",
        "    def __init__(self, batch, weight_by_gradient=True):\n",
        "        self.batch = batch\n",
        "        self.h = 192\n",
        "        self.w = 640\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.device(\"cuda\")\n",
        "        else:\n",
        "            self.device = torch.device(\"cpu\")\n",
        "\n",
        "        # For listing which frames to check\n",
        "        # For simple stereo is only current (0) and estimated pair (s)\n",
        "        self.frames = [0, 's']\n",
        "\n",
        "        # For multi-scale estimation\n",
        "        self.loss_scales = [0, 1, 2, 3]\n",
        "\n",
        "        # Weighting for loss function\n",
        "        self.alpha = 0.85\n",
        "\n",
        "        # Weighting for loss at high-contrast regions\n",
        "        self.weight_by_gradient = weight_by_gradient\n",
        "        self.beta = 0.1 \n",
        "\n",
        "        # Define projections for pose estimation\n",
        "        self.depth_projection = {}\n",
        "        self.projection_3d = {}\n",
        "        for loss_scale in self.loss_scales:\n",
        "            h = int(self.h / (2 ** loss_scale))\n",
        "            w = int(self.w / (2 ** loss_scale))\n",
        "\n",
        "            self.depth_projection[loss_scale] = monodepth2.layers.BackprojectDepth(self.batch, h, w).to(self.device)\n",
        "            self.projection_3d[loss_scale] = monodepth2.layers.Project3D(self.batch, h, w).to(self.device)\n",
        "        \n",
        "        ## Declare Depth Network\n",
        "        self.depth_encoder_network = monodepth2.networks.ResnetEncoder(18, True).to(self.device)\n",
        "        self.depth_decoder_network = monodepth2.networks.DepthDecoder(self.depth_encoder_network.num_ch_enc, \n",
        "                                                [0, 1, 2, 3]).to(self.device) \n",
        "\n",
        "        ## Declare Pose Network\n",
        "        self.pose_encoder_network = monodepth2.networks.ResnetEncoder(18, True, num_input_images=2).to(self.device)\n",
        "        self.pose_decoder_network = monodepth2.networks.PoseDecoder(self.pose_encoder_network.num_ch_enc, \n",
        "                                                num_input_features=1, num_frames_to_predict_for=2).to(self.device)\n",
        "\n",
        "        self.models = {\"encoder\": self.depth_encoder_network, \"depth\": self.depth_decoder_network,\n",
        "                       \"pose_encoder\": self.pose_encoder_network, \"pose\": self.pose_decoder_network}\n",
        "\n",
        "        # Set up our excerpt of Kitti Dataset\n",
        "        training_files = monodepth2.utils.readlines(\"/content/monodepth2/splits/eigen_full/train_files.txt\")\n",
        "        validation_files = monodepth2.utils.readlines(\"/content/monodepth2/splits/eigen_full/val_files.txt\")\n",
        "\n",
        "        train_set = monodepth2.datasets.kitti_dataset.KITTIRAWDataset(\"/content/kitti_data\", training_files, self.h, self.w, self.frames, 4, is_train=True, img_ext='.jpg')\n",
        "        self.train_loader = DataLoader(train_set, self.batch, True, num_workers=6, pin_memory=True, drop_last=True)\n",
        "        val_set = monodepth2.datasets.kitti_dataset.KITTIRAWDataset(\"/content/kitti_data\", validation_files, self.h, self.w, self.frames, 4, is_train=False, img_ext='.jpg')\n",
        "        self.val_loader = DataLoader(val_set, self.batch, True, num_workers=6, pin_memory=True, drop_last=True)\n",
        "\n",
        "    def run_training_loop(self):\n",
        "        self.all_params = list(self.depth_encoder_network.parameters())\n",
        "        self.all_params += list(self.depth_decoder_network.parameters())\n",
        "        self.all_params += list(self.pose_encoder_network.parameters())\n",
        "        self.all_params += list(self.pose_decoder_network.parameters())\n",
        "\n",
        "        # Same learning rate configuration as https://arxiv.org/pdf/1806.01260.pdf\n",
        "        self.adam_optim = optim.Adam(self.all_params, 1e-4)\n",
        "        self.lr_sched = optim.lr_scheduler.StepLR(self.adam_optim, 15, 0.1)\n",
        "        self.ssim_loss_func = monodepth2.layers.SSIM().to(self.device)\n",
        "\n",
        "        self.epoch_count = 0\n",
        "        self.losses  = []\n",
        "\n",
        "        for self.epoch_count in range(5):\n",
        "            # Per Epoch\n",
        "            self.lr_sched.step()\n",
        "            self.depth_encoder_network.train()\n",
        "            self.depth_decoder_network.train()\n",
        "            self.pose_encoder_network.train()\n",
        "            self.pose_decoder_network.train()\n",
        "\n",
        "            for idx, inputs in enumerate(self.train_loader):\n",
        "                # Push to GPU\n",
        "                for key, ipt in inputs.items():\n",
        "                    inputs[key] = ipt.to(self.device)\n",
        "\n",
        "                # Per Batch Code\n",
        "                batch_start_time = timeit.default_timer()\n",
        "\n",
        "                feature_identifications = self.depth_encoder_network(inputs[\"color_aug\", 0, 0])\n",
        "                outputs = self.depth_decoder_network(feature_identifications)\n",
        "\n",
        "                self.estimate_stereo_predictions(inputs, outputs)\n",
        "                loss = self.batch_loss_func(inputs, outputs)\n",
        "\n",
        "                self.adam_optim.zero_grad()\n",
        "                loss[\"loss\"].backward()\n",
        "                self.losses.append(loss[\"loss\"].item())\n",
        "                self.adam_optim.step()\n",
        "\n",
        "                batch_duration = timeit.default_timer() - batch_start_time\n",
        "\n",
        "                # Do something with batch duration and losses\n",
        "            \n",
        "            print(\"Finished Epoch: {} with Loss {}\".format(self.epoch_count, self.losses[-1]))\n",
        "\n",
        "    def estimate_stereo_predictions(self, inputs, outputs):\n",
        "        # Generate estimated stereo pair using the pose networks\n",
        "        for loss_scale in self.loss_scales:\n",
        "            estimated_disparity = outputs[(\"disp\", loss_scale)]\n",
        "            disp = F.interpolate(estimated_disparity, [self.h, self.w], mode=\"bilinear\", align_corners=False)\n",
        "            base = 0 # base scale\n",
        "\n",
        "            # Convert sigmoid disparity to depth estimate\n",
        "            scaled_disp = 0.001 + (10 - 0.001) * disp\n",
        "            depth = 1 / scaled_disp\n",
        "            outputs[(\"depth\", 0, loss_scale)] = depth\n",
        "\n",
        "            # Finalize Stereo Estimates\n",
        "            # Fetch camera extrinsics generated by KITTIRAWDataset\n",
        "            extrinsics = inputs[\"stereo_T\"]\n",
        "\n",
        "            camera_coords = self.depth_projection[base](depth, inputs[(\"inv_K\", base)])\n",
        "            pixel_coords = self.projection_3d[base](camera_coords, inputs[(\"K\", base)], extrinsics)\n",
        "\n",
        "            outputs[(\"sample\", 's', loss_scale)] = pixel_coords\n",
        "            outputs[(\"color\", 's', loss_scale)] = F.grid_sample(\n",
        "                inputs[(\"color\", 's', base)], outputs[(\"sample\", 's', loss_scale)], padding_mode=\"border\")\n",
        "                \n",
        "    def reprojection(self, prediction, target):\n",
        "        ssim_loss = self.ssim_loss_func(prediction, target).mean(1, True)\n",
        "        reproj_losses = self.alpha*ssim_loss + (1-self.alpha)*(torch.abs(target-prediction).mean(1, True))\n",
        "        return reproj_losses\n",
        "\n",
        "    def gradient_weighted_reprojection(self, prediction, target):\n",
        "        ssim_loss = self.ssim_loss_func(prediction, target).mean(1, True)\n",
        "\n",
        "        # Gradient to weight loss by\n",
        "        gradient = get_gradient(inputs[(\"color\", 0, loss_scale)], gauss_blur_sigma=2)\n",
        "\n",
        "        # Put gradient in the range [1, 1+beta] for beta < 1\n",
        "        gradient*=self.beta\n",
        "        gradient +=1\n",
        "        reproj_losses = self.alpha*ssim_loss + (1-self.alpha)*(torch.abs((target-prediction)*gradient).mean(1, True))\n",
        "        return reproj_losses\n",
        "\n",
        "    def batch_loss_func(self, inputs, outputs):\n",
        "        batch_loss = {}\n",
        "        complete_losses = 0\n",
        "\n",
        "        for loss_scale in self.loss_scales:\n",
        "            reproj_losses = []\n",
        "\n",
        "            if weight_by_gradient:\n",
        "                reproj_losses.append(self.gradient_weighted_reprojection(outputs[(\"color\", 's', loss_scale)], inputs[(\"color\", 0, 0)]))\n",
        "            else:\n",
        "                reproj_losses.append(self.reprojection(outputs[(\"color\", 's', loss_scale)], inputs[(\"color\", 0, 0)]))\n",
        "            reproj_losses = torch.cat(reproj_losses, 1)\n",
        "\n",
        "            identity_loss = []\n",
        "            identity_loss.append(self.reprojection(inputs[(\"color\", 's', 0)], inputs[(\"color\", 0, 0)]))\n",
        "            identity_loss = torch.cat(identity_loss, 1)\n",
        "\n",
        "\n",
        "            # Add some minor noise to ensure no repeated values\n",
        "            identity_loss += torch.rand(identity_loss.shape).cuda() * 0.00001\n",
        "            total = torch.cat((identity_loss, reproj_losses), dim=1)\n",
        "            val, idxs = torch.min(total, dim=1)\n",
        "            outputs[\"identity_selection/{}\".format(loss_scale)] = (idxs > identity_loss.shape[1] - 1).float()\n",
        "\n",
        "            current_loss = val.mean()\n",
        "\n",
        "            mean_disp = outputs[(\"disp\", loss_scale)].mean(2, True).mean(3, True)\n",
        "            norm_disp = outputs[(\"disp\", loss_scale)] / (mean_disp + 1e-7)\n",
        "            smooth_loss = monodepth2.layers.get_smooth_loss(norm_disp, inputs[(\"color\", 0, loss_scale)])\n",
        "\n",
        "            current_loss += 1e-3 * smooth_loss / (2 ** loss_scale)\n",
        "            complete_losses += current_loss\n",
        "            batch_loss[\"loss/{}\".format(loss_scale)] = current_loss\n",
        "\n",
        "        complete_losses /= len(self.loss_scales)\n",
        "        batch_loss[\"loss\"] = complete_losses\n",
        "        return batch_loss\n",
        "= current_loss\r\n",
        "        batch_loss[\"loss/{}\".format(scale)] = current_loss\r\n",
        "\r\n",
        "    total_loss /= len(self.loss_scales)\r\n",
        "    batch_loss[\"loss\"] = total_loss\r\n",
        "    return losses\r\n"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "ow5DP4v9zQEH",
        "outputId": "fd2a11a7-5a91-42ab-e8fc-60d42c667986"
      },
      "source": [
        "train = Training()\r\n",
        "train.run_training_loop()"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-145-08875b32bd0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-144-0aff193286ff>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m## Declare Pose Network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose_encoder_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResnetEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_input_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose_decoder_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDepthDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose_encoder_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_ch_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_input_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames_to_predict_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-142-6ff604b6965e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_layers, pretrained, num_input_images)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum_input_images\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet_multiimage_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_input_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'resnet_multiimage_input' is not defined"
          ]
        }
      ]
    },
    {
      "source": [
        "# References\n",
        "\n",
        "[1] https://arxiv.org/pdf/1806.01260.pdf\n",
        "\n",
        "[2] https://arxiv.org/pdf/1511.08861.pdf"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "uVMqZmbOPPeE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}